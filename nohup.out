Prepare micro.sleep ...
Exec script: /home/bench/HiBench/bin/workloads/micro/sleep/prepare/prepare.sh
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/micro/sleep.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start PrepareSleep bench
Prepare sleep: nothing to do
finish PrepareSleep bench
Run micro/sleep/hadoop
Exec script: /home/bench/HiBench/bin/workloads/micro/sleep/hadoop/run.sh
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/micro/sleep.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start HadoopSleep bench
Submit MapReduce Job: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop jar /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar sleep -m 8 -r 8 -mt 3 -mr 3
java.lang.NoClassDefFoundError: junit/framework/TestCase
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at org.apache.hadoop.test.MapredTestDriver.<init>(MapredTestDriver.java:109)
	at org.apache.hadoop.test.MapredTestDriver.<init>(MapredTestDriver.java:61)
	at org.apache.hadoop.test.MapredTestDriver.main(MapredTestDriver.java:147)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
Caused by: java.lang.ClassNotFoundException: junit.framework.TestCase
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 21 more
19/04/24 01:54:21 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 01:54:22 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 01:54:22 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/bench/.staging/job_1556063405565_0006
19/04/24 01:54:23 INFO mapreduce.JobSubmitter: number of splits:8
19/04/24 01:54:23 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1556063405565_0006
19/04/24 01:54:23 INFO mapreduce.JobSubmitter: Executing with tokens: []
19/04/24 01:54:23 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/04/24 01:54:23 INFO impl.YarnClientImpl: Submitted application application_1556063405565_0006
19/04/24 01:54:23 INFO mapreduce.Job: The url to track the job: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0006/
19/04/24 01:54:23 INFO mapreduce.Job: Running job: job_1556063405565_0006
19/04/24 01:54:30 INFO mapreduce.Job: Job job_1556063405565_0006 running in uber mode : false
19/04/24 01:54:30 INFO mapreduce.Job:  map 0% reduce 0%
19/04/24 01:54:38 INFO mapreduce.Job:  map 13% reduce 0%
19/04/24 01:54:40 INFO mapreduce.Job:  map 25% reduce 0%
19/04/24 01:54:41 INFO mapreduce.Job:  map 38% reduce 0%
19/04/24 01:54:42 INFO mapreduce.Job:  map 100% reduce 0%
19/04/24 01:54:44 INFO mapreduce.Job:  map 100% reduce 13%
19/04/24 01:54:45 INFO mapreduce.Job:  map 100% reduce 25%
19/04/24 01:54:46 INFO mapreduce.Job:  map 100% reduce 38%
19/04/24 01:54:48 INFO mapreduce.Job:  map 100% reduce 50%
19/04/24 01:54:49 INFO mapreduce.Job:  map 100% reduce 75%
19/04/24 01:54:51 INFO mapreduce.Job:  map 100% reduce 88%
19/04/24 01:54:53 INFO mapreduce.Job:  map 100% reduce 100%
19/04/24 01:54:53 INFO mapreduce.Job: Job job_1556063405565_0006 completed successfully
19/04/24 01:54:53 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=432
		FILE: Number of bytes written=3749016
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=384
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=8
		Launched reduce tasks=8
		Other local map tasks=8
		Total time spent by all maps in occupied slots (ms)=296120
		Total time spent by all reduces in occupied slots (ms)=160720
		Total time spent by all map tasks (ms)=74030
		Total time spent by all reduce tasks (ms)=20090
		Total vcore-milliseconds taken by all map tasks=74030
		Total vcore-milliseconds taken by all reduce tasks=20090
		Total megabyte-milliseconds taken by all map tasks=303226880
		Total megabyte-milliseconds taken by all reduce tasks=164577280
	Map-Reduce Framework
		Map input records=8
		Map output records=64
		Map output bytes=256
		Map output materialized bytes=768
		Input split bytes=384
		Combine input records=0
		Combine output records=0
		Reduce input groups=8
		Reduce shuffle bytes=768
		Reduce input records=64
		Reduce output records=0
		Spilled Records=128
		Shuffled Maps =64
		Failed Shuffles=0
		Merged Map outputs=64
		GC time elapsed (ms)=1522
		CPU time spent (ms)=54220
		Physical memory (bytes) snapshot=20837027840
		Virtual memory (bytes) snapshot=116931641344
		Total committed heap usage (bytes)=20348141568
		Peak Map Physical memory (bytes)=2382594048
		Peak Map Virtual memory (bytes)=5514493952
		Peak Reduce Physical memory (bytes)=229638144
		Peak Reduce Virtual memory (bytes)=9107820544
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
finish HadoopSleep bench
Run micro/sleep/spark
Exec script: /home/bench/HiBench/bin/workloads/micro/sleep/spark/run.sh
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/micro/sleep.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start ScalaSparkSleep bench
Export env: SPARKBENCH_PROPERTIES_FILES=/home/bench/HiBench/report/sleep/spark/conf/sparkbench/sparkbench.conf
Submit Spark job: /usr/hdp/current/spark2-client/bin/spark-submit  --properties-file /home/bench/HiBench/report/sleep/spark/conf/sparkbench/spark.conf --class com.intel.hibench.sparkbench.micro.ScalaSleep --master yarn  /home/bench/HiBench/sparkbench/assembly/target/sparkbench-assembly-7.1-SNAPSHOT-dist.jar 3
19/04/24 01:54:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/04/24 01:54:56 INFO SparkContext: Running Spark version 2.3.2.3.1.0.0-78
19/04/24 01:54:56 INFO SparkContext: Submitted application: ScalaSleep
19/04/24 01:54:56 INFO SecurityManager: Changing view acls to: bench
19/04/24 01:54:56 INFO SecurityManager: Changing modify acls to: bench
19/04/24 01:54:56 INFO SecurityManager: Changing view acls groups to: 
19/04/24 01:54:56 INFO SecurityManager: Changing modify acls groups to: 
19/04/24 01:54:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bench); groups with view permissions: Set(); users  with modify permissions: Set(bench); groups with modify permissions: Set()
19/04/24 01:54:56 INFO Utils: Successfully started service 'sparkDriver' on port 43607.
19/04/24 01:54:56 INFO SparkEnv: Registering MapOutputTracker
19/04/24 01:54:56 INFO SparkEnv: Registering BlockManagerMaster
19/04/24 01:54:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/04/24 01:54:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/04/24 01:54:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0740bdc4-e329-4311-ba0c-1dbca61086a4
19/04/24 01:54:56 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB
19/04/24 01:54:57 INFO SparkEnv: Registering OutputCommitCoordinator
19/04/24 01:54:57 INFO log: Logging initialized @2626ms
19/04/24 01:54:57 INFO Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T19:11:56+02:00, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
19/04/24 01:54:57 INFO Server: Started @2759ms
19/04/24 01:54:57 INFO AbstractConnector: Started ServerConnector@4628b1d3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19/04/24 01:54:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7adbd080{/jobs,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@67af833b{/jobs/json,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@d1f74b8{/jobs/job,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@21a5fd96{/jobs/job/json,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5769e7ae{/stages,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5c77053b{/stages/json,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@26b894bd{/stages/stage,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5489c777{/stages/stage/json,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3676ac27{/stages/pool,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@62f87c44{/stages/pool/json,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@48f5bde6{/storage,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@525d79f0{/storage/json,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5149f008{/storage/rdd,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7072bc39{/storage/rdd/json,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@158d255c{/environment,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2ca65ce4{/environment/json,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@327120c8{/executors,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5707c1cb{/executors/json,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2b5cb9b2{/executors/threadDump,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@35038141{/executors/threadDump/json,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@ecf9049{/static,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7a0e1b5e{/,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@702ed190{/api,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@70e29e14{/jobs/job/kill,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3b1bb3ab{/stages/stage/kill,null,AVAILABLE,@Spark}
19/04/24 01:54:57 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://varlet1.fyre.ibm.com:4040
19/04/24 01:54:57 INFO SparkContext: Added JAR file:/home/bench/HiBench/sparkbench/assembly/target/sparkbench-assembly-7.1-SNAPSHOT-dist.jar at spark://varlet1.fyre.ibm.com:43607/jars/sparkbench-assembly-7.1-SNAPSHOT-dist.jar with timestamp 1556063697402
19/04/24 01:54:58 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
19/04/24 01:54:58 INFO RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 01:54:58 INFO Client: Requesting a new application from cluster with 5 NodeManagers
19/04/24 01:54:58 INFO Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/04/24 01:54:58 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
19/04/24 01:54:58 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
19/04/24 01:54:58 INFO Client: Setting up container launch context for our AM
19/04/24 01:54:58 INFO Client: Setting up the launch environment for our AM container
19/04/24 01:54:58 INFO Client: Preparing resources for our AM container
19/04/24 01:55:00 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://a1.fyre.ibm.com:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-yarn-archive.tar.gz
19/04/24 01:55:00 INFO Client: Source and destination file systems are the same. Not copying hdfs://a1.fyre.ibm.com:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-yarn-archive.tar.gz
19/04/24 01:55:00 INFO Client: Distribute hdfs cache file as spark.sql.hive.metastore.jars for HDP, hdfsCacheFile:hdfs://a1.fyre.ibm.com:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-hive-archive.tar.gz
19/04/24 01:55:00 INFO Client: Source and destination file systems are the same. Not copying hdfs://a1.fyre.ibm.com:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-hive-archive.tar.gz
19/04/24 01:55:00 INFO Client: Uploading resource file:/tmp/spark-83b00e4a-44c8-4ad0-bd3b-c6d1fdcbcf4e/__spark_conf__3429860503621403294.zip -> hdfs://a1.fyre.ibm.com:8020/user/bench/.sparkStaging/application_1556063405565_0007/__spark_conf__.zip
19/04/24 01:55:01 INFO SecurityManager: Changing view acls to: bench
19/04/24 01:55:01 INFO SecurityManager: Changing modify acls to: bench
19/04/24 01:55:01 INFO SecurityManager: Changing view acls groups to: 
19/04/24 01:55:01 INFO SecurityManager: Changing modify acls groups to: 
19/04/24 01:55:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bench); groups with view permissions: Set(); users  with modify permissions: Set(bench); groups with modify permissions: Set()
19/04/24 01:55:01 INFO Client: Submitting application application_1556063405565_0007 to ResourceManager
19/04/24 01:55:01 INFO YarnClientImpl: Submitted application application_1556063405565_0007
19/04/24 01:55:01 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1556063405565_0007 and attemptId None
19/04/24 01:55:02 INFO Client: Application report for application_1556063405565_0007 (state: ACCEPTED)
19/04/24 01:55:02 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1556063701215
	 final status: UNDEFINED
	 tracking URL: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0007/
	 user: bench
19/04/24 01:55:03 INFO Client: Application report for application_1556063405565_0007 (state: ACCEPTED)
19/04/24 01:55:04 INFO Client: Application report for application_1556063405565_0007 (state: ACCEPTED)
19/04/24 01:55:04 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> a1.fyre.ibm.com, PROXY_URI_BASES -> http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0007), /proxy/application_1556063405565_0007
19/04/24 01:55:04 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.
19/04/24 01:55:05 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
19/04/24 01:55:05 INFO Client: Application report for application_1556063405565_0007 (state: RUNNING)
19/04/24 01:55:05 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.16.191.118
	 ApplicationMaster RPC port: 0
	 queue: default
	 start time: 1556063701215
	 final status: UNDEFINED
	 tracking URL: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0007/
	 user: bench
19/04/24 01:55:05 INFO YarnClientSchedulerBackend: Application application_1556063405565_0007 has started running.
19/04/24 01:55:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36820.
19/04/24 01:55:05 INFO NettyBlockTransferService: Server created on varlet1.fyre.ibm.com:36820
19/04/24 01:55:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/04/24 01:55:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, varlet1.fyre.ibm.com, 36820, None)
19/04/24 01:55:05 INFO BlockManagerMasterEndpoint: Registering block manager varlet1.fyre.ibm.com:36820 with 2004.6 MB RAM, BlockManagerId(driver, varlet1.fyre.ibm.com, 36820, None)
19/04/24 01:55:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, varlet1.fyre.ibm.com, 36820, None)
19/04/24 01:55:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, varlet1.fyre.ibm.com, 36820, None)
19/04/24 01:55:05 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.
19/04/24 01:55:05 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@526f6427{/metrics/json,null,AVAILABLE,@Spark}
19/04/24 01:55:07 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.191.74:58232) with ID 2
19/04/24 01:55:07 INFO BlockManagerMasterEndpoint: Registering block manager hurds3.fyre.ibm.com:35016 with 2004.6 MB RAM, BlockManagerId(2, hurds3.fyre.ibm.com, 35016, None)
19/04/24 01:55:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.191.89:35234) with ID 1
19/04/24 01:55:10 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
19/04/24 01:55:10 INFO BlockManagerMasterEndpoint: Registering block manager hurds4.fyre.ibm.com:34699 with 2004.6 MB RAM, BlockManagerId(1, hurds4.fyre.ibm.com, 34699, None)
19/04/24 01:55:11 INFO SparkContext: Starting job: collect at ScalaSleep.scala:36
19/04/24 01:55:11 INFO DAGScheduler: Got job 0 (collect at ScalaSleep.scala:36) with 8 output partitions
19/04/24 01:55:11 INFO DAGScheduler: Final stage: ResultStage 0 (collect at ScalaSleep.scala:36)
19/04/24 01:55:11 INFO DAGScheduler: Parents of final stage: List()
19/04/24 01:55:11 INFO DAGScheduler: Missing parents: List()
19/04/24 01:55:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ScalaSleep.scala:35), which has no missing parents
19/04/24 01:55:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.0 KB, free 2004.6 MB)
19/04/24 01:55:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1334.0 B, free 2004.6 MB)
19/04/24 01:55:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on varlet1.fyre.ibm.com:36820 (size: 1334.0 B, free: 2004.6 MB)
19/04/24 01:55:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1039
19/04/24 01:55:11 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ScalaSleep.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
19/04/24 01:55:11 INFO YarnScheduler: Adding task set 0.0 with 8 tasks
19/04/24 01:55:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hurds4.fyre.ibm.com, executor 1, partition 0, PROCESS_LOCAL, 7864 bytes)
19/04/24 01:55:11 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hurds3.fyre.ibm.com, executor 2, partition 1, PROCESS_LOCAL, 7864 bytes)
19/04/24 01:55:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hurds4.fyre.ibm.com:34699 (size: 1334.0 B, free: 2004.6 MB)
19/04/24 01:55:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hurds3.fyre.ibm.com:35016 (size: 1334.0 B, free: 2004.6 MB)
19/04/24 01:55:15 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hurds4.fyre.ibm.com, executor 1, partition 2, PROCESS_LOCAL, 7864 bytes)
19/04/24 01:55:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3933 ms on hurds4.fyre.ibm.com (executor 1) (1/8)
19/04/24 01:55:15 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hurds3.fyre.ibm.com, executor 2, partition 3, PROCESS_LOCAL, 7864 bytes)
19/04/24 01:55:15 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3916 ms on hurds3.fyre.ibm.com (executor 2) (2/8)
19/04/24 01:55:18 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, hurds4.fyre.ibm.com, executor 1, partition 4, PROCESS_LOCAL, 7864 bytes)
19/04/24 01:55:18 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 3038 ms on hurds4.fyre.ibm.com (executor 1) (3/8)
19/04/24 01:55:18 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, hurds3.fyre.ibm.com, executor 2, partition 5, PROCESS_LOCAL, 7864 bytes)
19/04/24 01:55:18 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 3027 ms on hurds3.fyre.ibm.com (executor 2) (4/8)
19/04/24 01:55:21 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, hurds4.fyre.ibm.com, executor 1, partition 6, PROCESS_LOCAL, 7864 bytes)
19/04/24 01:55:21 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 3025 ms on hurds4.fyre.ibm.com (executor 1) (5/8)
19/04/24 01:55:21 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, hurds3.fyre.ibm.com, executor 2, partition 7, PROCESS_LOCAL, 7921 bytes)
19/04/24 01:55:21 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 3026 ms on hurds3.fyre.ibm.com (executor 2) (6/8)
19/04/24 01:55:24 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 3022 ms on hurds4.fyre.ibm.com (executor 1) (7/8)
19/04/24 01:55:24 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 3032 ms on hurds3.fyre.ibm.com (executor 2) (8/8)
19/04/24 01:55:24 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
19/04/24 01:55:24 INFO DAGScheduler: ResultStage 0 (collect at ScalaSleep.scala:36) finished in 13,251 s
19/04/24 01:55:24 INFO DAGScheduler: Job 0 finished: collect at ScalaSleep.scala:36, took 13,325511 s
19/04/24 01:55:24 INFO AbstractConnector: Stopped Spark@4628b1d3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19/04/24 01:55:24 INFO SparkUI: Stopped Spark web UI at http://varlet1.fyre.ibm.com:4040
19/04/24 01:55:24 INFO YarnClientSchedulerBackend: Interrupting monitor thread
19/04/24 01:55:24 INFO YarnClientSchedulerBackend: Shutting down all executors
19/04/24 01:55:24 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
19/04/24 01:55:24 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
19/04/24 01:55:24 INFO YarnClientSchedulerBackend: Stopped
19/04/24 01:55:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/04/24 01:55:24 INFO MemoryStore: MemoryStore cleared
19/04/24 01:55:24 INFO BlockManager: BlockManager stopped
19/04/24 01:55:24 INFO BlockManagerMaster: BlockManagerMaster stopped
19/04/24 01:55:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/04/24 01:55:24 INFO SparkContext: Successfully stopped SparkContext
19/04/24 01:55:24 INFO ShutdownHookManager: Shutdown hook called
19/04/24 01:55:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-83b00e4a-44c8-4ad0-bd3b-c6d1fdcbcf4e
19/04/24 01:55:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-68c06476-79af-4b58-a3a1-9d779cd44ac6
finish ScalaSparkSleep bench
Prepare micro.sort ...
Exec script: /home/bench/HiBench/bin/workloads/micro/sort/prepare/prepare.sh
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/micro/sort.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start HadoopPrepareSort bench
hdfs rm -r: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -rm -r -skipTrash hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Sort/Input
rm: `hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Sort/Input': No such file or directory
Submit MapReduce Job: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop jar /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-examples.jar randomtextwriter -D mapreduce.randomtextwriter.totalbytes=32000 -D mapreduce.randomtextwriter.bytespermap=4000 -D mapreduce.job.maps=8 -D mapreduce.job.reduces=8 hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Sort/Input
19/04/24 01:55:29 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 01:55:29 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
Running 8 maps.
Job started: Wed Apr 24 01:55:30 CEST 2019
19/04/24 01:55:30 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 01:55:30 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 01:55:31 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/bench/.staging/job_1556063405565_0008
19/04/24 01:55:31 INFO mapreduce.JobSubmitter: number of splits:8
19/04/24 01:55:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1556063405565_0008
19/04/24 01:55:31 INFO mapreduce.JobSubmitter: Executing with tokens: []
19/04/24 01:55:32 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/04/24 01:55:32 INFO impl.YarnClientImpl: Submitted application application_1556063405565_0008
19/04/24 01:55:32 INFO mapreduce.Job: The url to track the job: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0008/
19/04/24 01:55:32 INFO mapreduce.Job: Running job: job_1556063405565_0008
19/04/24 01:55:38 INFO mapreduce.Job: Job job_1556063405565_0008 running in uber mode : false
19/04/24 01:55:38 INFO mapreduce.Job:  map 0% reduce 0%
19/04/24 01:55:43 INFO mapreduce.Job:  map 38% reduce 0%
19/04/24 01:55:44 INFO mapreduce.Job:  map 100% reduce 0%
19/04/24 01:55:44 INFO mapreduce.Job: Job job_1556063405565_0008 completed successfully
19/04/24 01:55:44 INFO mapreduce.Job: Counters: 34
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=1859184
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1096
		HDFS: Number of bytes written=36982
		HDFS: Number of read operations=48
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=16
	Job Counters 
		Launched map tasks=8
		Other local map tasks=8
		Total time spent by all maps in occupied slots (ms)=107132
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=26783
		Total vcore-milliseconds taken by all map tasks=26783
		Total megabyte-milliseconds taken by all map tasks=109703168
	Map-Reduce Framework
		Map input records=8
		Map output records=56
		Input split bytes=1096
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=1007
		CPU time spent (ms)=5510
		Physical memory (bytes) snapshot=1821458432
		Virtual memory (bytes) snapshot=44103573504
		Total committed heap usage (bytes)=1586495488
		Peak Map Physical memory (bytes)=231612416
		Peak Map Virtual memory (bytes)=5518057472
	org.apache.hadoop.examples.RandomTextWriter$Counters
		BYTES_WRITTEN=35698
		RECORDS_WRITTEN=56
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=36982
Job ended: Wed Apr 24 01:55:44 CEST 2019
The job took 13 seconds.
finish HadoopPrepareSort bench
Run micro/sort/hadoop
Exec script: /home/bench/HiBench/bin/workloads/micro/sort/hadoop/run.sh
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/micro/sort.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start HadoopSort bench
hdfs rm -r: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -rm -r -skipTrash hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Sort/Output
rm: `hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Sort/Output': No such file or directory
hdfs du -s: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -du -s hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Sort/Input
Submit MapReduce Job: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop jar /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-examples.jar sort -outKey org.apache.hadoop.io.Text -outValue org.apache.hadoop.io.Text -r 8 hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Sort/Input hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Sort/Output
19/04/24 01:55:52 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 01:55:52 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
Running on 5 nodes to sort from hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Sort/Input into hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Sort/Output with 8 reduces.
Job started: Wed Apr 24 01:55:53 CEST 2019
19/04/24 01:55:53 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 01:55:53 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 01:55:53 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/bench/.staging/job_1556063405565_0009
19/04/24 01:55:54 INFO input.FileInputFormat: Total input files to process : 8
19/04/24 01:55:54 INFO mapreduce.JobSubmitter: number of splits:8
19/04/24 01:55:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1556063405565_0009
19/04/24 01:55:54 INFO mapreduce.JobSubmitter: Executing with tokens: []
19/04/24 01:55:54 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/04/24 01:55:54 INFO impl.YarnClientImpl: Submitted application application_1556063405565_0009
19/04/24 01:55:54 INFO mapreduce.Job: The url to track the job: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0009/
19/04/24 01:55:54 INFO mapreduce.Job: Running job: job_1556063405565_0009
19/04/24 01:56:02 INFO mapreduce.Job: Job job_1556063405565_0009 running in uber mode : false
19/04/24 01:56:02 INFO mapreduce.Job:  map 0% reduce 0%
19/04/24 01:56:09 INFO mapreduce.Job:  map 25% reduce 0%
19/04/24 01:56:10 INFO mapreduce.Job:  map 63% reduce 0%
19/04/24 01:56:11 INFO mapreduce.Job:  map 100% reduce 0%
19/04/24 01:56:14 INFO mapreduce.Job:  map 100% reduce 13%
19/04/24 01:56:15 INFO mapreduce.Job:  map 100% reduce 25%
19/04/24 01:56:18 INFO mapreduce.Job:  map 100% reduce 50%
19/04/24 01:56:22 INFO mapreduce.Job:  map 100% reduce 75%
19/04/24 01:56:25 INFO mapreduce.Job:  map 100% reduce 88%
19/04/24 01:56:26 INFO mapreduce.Job:  map 100% reduce 100%
19/04/24 01:56:27 INFO mapreduce.Job: Job job_1556063405565_0009 completed successfully
19/04/24 01:56:27 INFO mapreduce.Job: Counters: 54
	File System Counters
		FILE: Number of bytes read=36170
		FILE: Number of bytes written=3798380
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=38070
		HDFS: Number of bytes written=36982
		HDFS: Number of read operations=72
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=16
	Job Counters 
		Launched map tasks=8
		Launched reduce tasks=8
		Data-local map tasks=7
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=185328
		Total time spent by all reduces in occupied slots (ms)=173240
		Total time spent by all map tasks (ms)=46332
		Total time spent by all reduce tasks (ms)=21655
		Total vcore-milliseconds taken by all map tasks=46332
		Total vcore-milliseconds taken by all reduce tasks=21655
		Total megabyte-milliseconds taken by all map tasks=189775872
		Total megabyte-milliseconds taken by all reduce tasks=177397760
	Map-Reduce Framework
		Map input records=56
		Map output records=56
		Map output bytes=35910
		Map output materialized bytes=36506
		Input split bytes=1088
		Combine input records=0
		Combine output records=0
		Reduce input groups=56
		Reduce shuffle bytes=36506
		Reduce input records=56
		Reduce output records=56
		Spilled Records=112
		Shuffled Maps =64
		Failed Shuffles=0
		Merged Map outputs=64
		GC time elapsed (ms)=1638
		CPU time spent (ms)=34110
		Physical memory (bytes) snapshot=20844785664
		Virtual memory (bytes) snapshot=116955226112
		Total committed heap usage (bytes)=20301479936
		Peak Map Physical memory (bytes)=2380181504
		Peak Map Virtual memory (bytes)=5515997184
		Peak Reduce Physical memory (bytes)=233361408
		Peak Reduce Virtual memory (bytes)=9110122496
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=36982
	File Output Format Counters 
		Bytes Written=36982
Job ended: Wed Apr 24 01:56:27 CEST 2019
The job took 34 seconds.
finish HadoopSort bench
Run micro/sort/spark
Exec script: /home/bench/HiBench/bin/workloads/micro/sort/spark/run.sh
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/micro/sort.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start ScalaSparkSort bench
hdfs rm -r: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -rm -r -skipTrash hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Sort/Output
Deleted hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Sort/Output
hdfs du -s: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -du -s hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Sort/Input
Export env: SPARKBENCH_PROPERTIES_FILES=/home/bench/HiBench/report/sort/spark/conf/sparkbench/sparkbench.conf
Submit Spark job: /usr/hdp/current/spark2-client/bin/spark-submit  --properties-file /home/bench/HiBench/report/sort/spark/conf/sparkbench/spark.conf --class com.intel.hibench.sparkbench.micro.ScalaSort --master yarn  /home/bench/HiBench/sparkbench/assembly/target/sparkbench-assembly-7.1-SNAPSHOT-dist.jar hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Sort/Input hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Sort/Output
19/04/24 01:56:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/04/24 01:56:36 INFO SparkContext: Running Spark version 2.3.2.3.1.0.0-78
19/04/24 01:56:36 INFO SparkContext: Submitted application: ScalaSort
19/04/24 01:56:36 INFO SecurityManager: Changing view acls to: bench
19/04/24 01:56:36 INFO SecurityManager: Changing modify acls to: bench
19/04/24 01:56:36 INFO SecurityManager: Changing view acls groups to: 
19/04/24 01:56:36 INFO SecurityManager: Changing modify acls groups to: 
19/04/24 01:56:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bench); groups with view permissions: Set(); users  with modify permissions: Set(bench); groups with modify permissions: Set()
19/04/24 01:56:36 INFO Utils: Successfully started service 'sparkDriver' on port 33792.
19/04/24 01:56:36 INFO SparkEnv: Registering MapOutputTracker
19/04/24 01:56:36 INFO SparkEnv: Registering BlockManagerMaster
19/04/24 01:56:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/04/24 01:56:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/04/24 01:56:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5e283899-e218-4b26-88fb-538046649be8
19/04/24 01:56:36 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB
19/04/24 01:56:36 INFO SparkEnv: Registering OutputCommitCoordinator
19/04/24 01:56:36 INFO log: Logging initialized @2896ms
19/04/24 01:56:37 INFO Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T19:11:56+02:00, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
19/04/24 01:56:37 INFO Server: Started @3024ms
19/04/24 01:56:37 INFO AbstractConnector: Started ServerConnector@63f34b70{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19/04/24 01:56:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@c00fff0{/jobs,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@48f5bde6{/jobs/json,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@525d79f0{/jobs/job,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@158d255c{/jobs/job/json,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2ca65ce4{/stages,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@327120c8{/stages/json,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5707c1cb{/stages/stage,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@ecf9049{/stages/stage/json,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@672f11c2{/stages/pool,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2970a5bc{/stages/pool/json,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@50305a{/storage,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@72efb5c1{/storage/json,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6d511b5f{/storage/rdd,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@41200e0c{/storage/rdd/json,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@40f33492{/environment,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4fbdc0f0{/environment/json,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2ad3a1bb{/executors,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6bc28a83{/executors/json,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@324c64cd{/executors/threadDump,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@13579834{/executors/threadDump/json,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@24be2d9c{/static,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@37d80fe7{/,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@384fc774{/api,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@408b35bf{/jobs/job/kill,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@29ad44e3{/stages/stage/kill,null,AVAILABLE,@Spark}
19/04/24 01:56:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://varlet1.fyre.ibm.com:4040
19/04/24 01:56:37 INFO SparkContext: Added JAR file:/home/bench/HiBench/sparkbench/assembly/target/sparkbench-assembly-7.1-SNAPSHOT-dist.jar at spark://varlet1.fyre.ibm.com:33792/jars/sparkbench-assembly-7.1-SNAPSHOT-dist.jar with timestamp 1556063797216
19/04/24 01:56:38 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
19/04/24 01:56:38 INFO RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 01:56:38 INFO Client: Requesting a new application from cluster with 5 NodeManagers
19/04/24 01:56:38 INFO Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/04/24 01:56:38 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
19/04/24 01:56:38 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
19/04/24 01:56:38 INFO Client: Setting up container launch context for our AM
19/04/24 01:56:38 INFO Client: Setting up the launch environment for our AM container
19/04/24 01:56:38 INFO Client: Preparing resources for our AM container
19/04/24 01:56:40 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://a1.fyre.ibm.com:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-yarn-archive.tar.gz
19/04/24 01:56:40 INFO Client: Source and destination file systems are the same. Not copying hdfs://a1.fyre.ibm.com:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-yarn-archive.tar.gz
19/04/24 01:56:40 INFO Client: Distribute hdfs cache file as spark.sql.hive.metastore.jars for HDP, hdfsCacheFile:hdfs://a1.fyre.ibm.com:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-hive-archive.tar.gz
19/04/24 01:56:40 INFO Client: Source and destination file systems are the same. Not copying hdfs://a1.fyre.ibm.com:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-hive-archive.tar.gz
19/04/24 01:56:40 INFO Client: Uploading resource file:/tmp/spark-9db2093e-3658-44d8-88d7-ea933c01e1b9/__spark_conf__5465869901546852394.zip -> hdfs://a1.fyre.ibm.com:8020/user/bench/.sparkStaging/application_1556063405565_0010/__spark_conf__.zip
19/04/24 01:56:40 INFO SecurityManager: Changing view acls to: bench
19/04/24 01:56:40 INFO SecurityManager: Changing modify acls to: bench
19/04/24 01:56:40 INFO SecurityManager: Changing view acls groups to: 
19/04/24 01:56:40 INFO SecurityManager: Changing modify acls groups to: 
19/04/24 01:56:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bench); groups with view permissions: Set(); users  with modify permissions: Set(bench); groups with modify permissions: Set()
19/04/24 01:56:40 INFO Client: Submitting application application_1556063405565_0010 to ResourceManager
19/04/24 01:56:41 INFO YarnClientImpl: Submitted application application_1556063405565_0010
19/04/24 01:56:41 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1556063405565_0010 and attemptId None
19/04/24 01:56:42 INFO Client: Application report for application_1556063405565_0010 (state: ACCEPTED)
19/04/24 01:56:42 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1556063801021
	 final status: UNDEFINED
	 tracking URL: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0010/
	 user: bench
19/04/24 01:56:43 INFO Client: Application report for application_1556063405565_0010 (state: ACCEPTED)
19/04/24 01:56:43 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> a1.fyre.ibm.com, PROXY_URI_BASES -> http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0010), /proxy/application_1556063405565_0010
19/04/24 01:56:43 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.
19/04/24 01:56:44 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
19/04/24 01:56:44 INFO Client: Application report for application_1556063405565_0010 (state: RUNNING)
19/04/24 01:56:44 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.16.191.74
	 ApplicationMaster RPC port: 0
	 queue: default
	 start time: 1556063801021
	 final status: UNDEFINED
	 tracking URL: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0010/
	 user: bench
19/04/24 01:56:44 INFO YarnClientSchedulerBackend: Application application_1556063405565_0010 has started running.
19/04/24 01:56:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34440.
19/04/24 01:56:44 INFO NettyBlockTransferService: Server created on varlet1.fyre.ibm.com:34440
19/04/24 01:56:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/04/24 01:56:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, varlet1.fyre.ibm.com, 34440, None)
19/04/24 01:56:44 INFO BlockManagerMasterEndpoint: Registering block manager varlet1.fyre.ibm.com:34440 with 2004.6 MB RAM, BlockManagerId(driver, varlet1.fyre.ibm.com, 34440, None)
19/04/24 01:56:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, varlet1.fyre.ibm.com, 34440, None)
19/04/24 01:56:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, varlet1.fyre.ibm.com, 34440, None)
19/04/24 01:56:44 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.
19/04/24 01:56:44 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4c18516{/metrics/json,null,AVAILABLE,@Spark}
19/04/24 01:56:47 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.191.17:33974) with ID 1
19/04/24 01:56:47 INFO BlockManagerMasterEndpoint: Registering block manager hurds2.fyre.ibm.com:43352 with 2004.6 MB RAM, BlockManagerId(1, hurds2.fyre.ibm.com, 43352, None)
19/04/24 01:56:47 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.191.118:34728) with ID 2
19/04/24 01:56:47 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
19/04/24 01:56:47 INFO BlockManagerMasterEndpoint: Registering block manager hurds5.fyre.ibm.com:43537 with 2004.6 MB RAM, BlockManagerId(2, hurds5.fyre.ibm.com, 43537, None)
19/04/24 01:56:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 353.3 KB, free 2004.3 MB)
19/04/24 01:56:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KB, free 2004.2 MB)
19/04/24 01:56:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on varlet1.fyre.ibm.com:34440 (size: 32.5 KB, free: 2004.6 MB)
19/04/24 01:56:48 INFO SparkContext: Created broadcast 0 from sequenceFile at IOCommon.scala:44
19/04/24 01:56:48 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
19/04/24 01:56:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
19/04/24 01:56:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
19/04/24 01:56:48 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78
19/04/24 01:56:48 INFO FileInputFormat: Total input files to process : 8
19/04/24 01:56:48 INFO DAGScheduler: Registering RDD 3 (map at ScalaSort.scala:47)
19/04/24 01:56:48 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:78) with 8 output partitions
19/04/24 01:56:48 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:78)
19/04/24 01:56:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
19/04/24 01:56:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
19/04/24 01:56:48 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at ScalaSort.scala:47), which has no missing parents
19/04/24 01:56:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.1 KB, free 2004.2 MB)
19/04/24 01:56:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.0 KB, free 2004.2 MB)
19/04/24 01:56:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on varlet1.fyre.ibm.com:34440 (size: 3.0 KB, free: 2004.6 MB)
19/04/24 01:56:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1039
19/04/24 01:56:48 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at ScalaSort.scala:47) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
19/04/24 01:56:48 INFO YarnScheduler: Adding task set 0.0 with 8 tasks
19/04/24 01:56:48 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 0, hurds2.fyre.ibm.com, executor 1, partition 1, NODE_LOCAL, 7918 bytes)
19/04/24 01:56:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 1, hurds5.fyre.ibm.com, executor 2, partition 0, NODE_LOCAL, 7918 bytes)
19/04/24 01:56:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hurds2.fyre.ibm.com:43352 (size: 3.0 KB, free: 2004.6 MB)
19/04/24 01:56:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hurds2.fyre.ibm.com:43352 (size: 32.5 KB, free: 2004.6 MB)
19/04/24 01:56:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hurds5.fyre.ibm.com:43537 (size: 3.0 KB, free: 2004.6 MB)
19/04/24 01:56:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hurds5.fyre.ibm.com:43537 (size: 32.5 KB, free: 2004.6 MB)
19/04/24 01:56:51 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 2, hurds2.fyre.ibm.com, executor 1, partition 3, NODE_LOCAL, 7918 bytes)
19/04/24 01:56:51 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 0) in 2644 ms on hurds2.fyre.ibm.com (executor 1) (1/8)
19/04/24 01:56:51 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 3, hurds5.fyre.ibm.com, executor 2, partition 2, NODE_LOCAL, 7918 bytes)
19/04/24 01:56:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 1) in 2628 ms on hurds5.fyre.ibm.com (executor 2) (2/8)
19/04/24 01:56:51 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, hurds2.fyre.ibm.com, executor 1, partition 4, NODE_LOCAL, 7918 bytes)
19/04/24 01:56:51 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 2) in 88 ms on hurds2.fyre.ibm.com (executor 1) (3/8)
19/04/24 01:56:51 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 5, hurds5.fyre.ibm.com, executor 2, partition 7, NODE_LOCAL, 7918 bytes)
19/04/24 01:56:51 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 3) in 76 ms on hurds5.fyre.ibm.com (executor 2) (4/8)
19/04/24 01:56:51 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 6, hurds2.fyre.ibm.com, executor 1, partition 5, NODE_LOCAL, 7918 bytes)
19/04/24 01:56:51 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 53 ms on hurds2.fyre.ibm.com (executor 1) (5/8)
19/04/24 01:56:51 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 5) in 55 ms on hurds5.fyre.ibm.com (executor 2) (6/8)
19/04/24 01:56:51 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 7, hurds2.fyre.ibm.com, executor 1, partition 6, NODE_LOCAL, 7918 bytes)
19/04/24 01:56:51 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 6) in 39 ms on hurds2.fyre.ibm.com (executor 1) (7/8)
19/04/24 01:56:51 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 7) in 46 ms on hurds2.fyre.ibm.com (executor 1) (8/8)
19/04/24 01:56:51 INFO DAGScheduler: ShuffleMapStage 0 (map at ScalaSort.scala:47) finished in 3,076 s
19/04/24 01:56:51 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
19/04/24 01:56:51 INFO DAGScheduler: looking for newly runnable stages
19/04/24 01:56:51 INFO DAGScheduler: running: Set()
19/04/24 01:56:51 INFO DAGScheduler: waiting: Set(ResultStage 1)
19/04/24 01:56:51 INFO DAGScheduler: failed: Set()
19/04/24 01:56:51 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at map at IOCommon.scala:61), which has no missing parents
19/04/24 01:56:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 94.6 KB, free 2004.1 MB)
19/04/24 01:56:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 35.4 KB, free 2004.1 MB)
19/04/24 01:56:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on varlet1.fyre.ibm.com:34440 (size: 35.4 KB, free: 2004.5 MB)
19/04/24 01:56:51 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1039
19/04/24 01:56:51 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at map at IOCommon.scala:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
19/04/24 01:56:51 INFO YarnScheduler: Adding task set 1.0 with 8 tasks
19/04/24 01:56:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, hurds5.fyre.ibm.com, executor 2, partition 0, NODE_LOCAL, 7660 bytes)
19/04/24 01:56:51 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 9, hurds2.fyre.ibm.com, executor 1, partition 1, NODE_LOCAL, 7660 bytes)
19/04/24 01:56:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hurds2.fyre.ibm.com:43352 (size: 35.4 KB, free: 2004.5 MB)
19/04/24 01:56:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hurds5.fyre.ibm.com:43537 (size: 35.4 KB, free: 2004.5 MB)
19/04/24 01:56:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.16.191.17:33974
19/04/24 01:56:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.16.191.118:34728
19/04/24 01:56:52 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 10, hurds2.fyre.ibm.com, executor 1, partition 2, NODE_LOCAL, 7660 bytes)
19/04/24 01:56:52 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 9) in 423 ms on hurds2.fyre.ibm.com (executor 1) (1/8)
19/04/24 01:56:52 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 11, hurds5.fyre.ibm.com, executor 2, partition 4, NODE_LOCAL, 7660 bytes)
19/04/24 01:56:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 8) in 437 ms on hurds5.fyre.ibm.com (executor 2) (2/8)
19/04/24 01:56:52 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 12, hurds2.fyre.ibm.com, executor 1, partition 3, NODE_LOCAL, 7660 bytes)
19/04/24 01:56:52 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 10) in 136 ms on hurds2.fyre.ibm.com (executor 1) (3/8)
19/04/24 01:56:52 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 13, hurds5.fyre.ibm.com, executor 2, partition 5, NODE_LOCAL, 7660 bytes)
19/04/24 01:56:52 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 11) in 132 ms on hurds5.fyre.ibm.com (executor 2) (4/8)
19/04/24 01:56:52 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 14, hurds2.fyre.ibm.com, executor 1, partition 6, NODE_LOCAL, 7660 bytes)
19/04/24 01:56:52 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 12) in 90 ms on hurds2.fyre.ibm.com (executor 1) (5/8)
19/04/24 01:56:52 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 13) in 142 ms on hurds5.fyre.ibm.com (executor 2) (6/8)
19/04/24 01:56:52 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 15, hurds2.fyre.ibm.com, executor 1, partition 7, NODE_LOCAL, 7660 bytes)
19/04/24 01:56:52 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 14) in 96 ms on hurds2.fyre.ibm.com (executor 1) (7/8)
19/04/24 01:56:52 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 15) in 90 ms on hurds2.fyre.ibm.com (executor 1) (8/8)
19/04/24 01:56:52 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
19/04/24 01:56:52 INFO DAGScheduler: ResultStage 1 (runJob at SparkHadoopWriter.scala:78) finished in 0,863 s
19/04/24 01:56:52 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 4,173653 s
19/04/24 01:56:52 INFO SparkHadoopWriter: Job job_20190424015648_0006 committed.
19/04/24 01:56:52 INFO AbstractConnector: Stopped Spark@63f34b70{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19/04/24 01:56:52 INFO SparkUI: Stopped Spark web UI at http://varlet1.fyre.ibm.com:4040
19/04/24 01:56:52 INFO YarnClientSchedulerBackend: Interrupting monitor thread
19/04/24 01:56:52 INFO YarnClientSchedulerBackend: Shutting down all executors
19/04/24 01:56:52 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
19/04/24 01:56:52 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
19/04/24 01:56:52 INFO YarnClientSchedulerBackend: Stopped
19/04/24 01:56:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/04/24 01:56:52 INFO MemoryStore: MemoryStore cleared
19/04/24 01:56:52 INFO BlockManager: BlockManager stopped
19/04/24 01:56:52 INFO BlockManagerMaster: BlockManagerMaster stopped
19/04/24 01:56:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/04/24 01:56:52 INFO SparkContext: Successfully stopped SparkContext
19/04/24 01:56:52 INFO ShutdownHookManager: Shutdown hook called
19/04/24 01:56:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-12b7bd2e-805d-40f1-bfa2-80f095cde017
19/04/24 01:56:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-9db2093e-3658-44d8-88d7-ea933c01e1b9
finish ScalaSparkSort bench
Prepare micro.terasort ...
Exec script: /home/bench/HiBench/bin/workloads/micro/terasort/prepare/prepare.sh
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/micro/terasort.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start HadoopPrepareTerasort bench
hdfs rm -r: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -rm -r -skipTrash hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Terasort/Input
rm: `hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Terasort/Input': No such file or directory
Submit MapReduce Job: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop jar /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen -D mapreduce.job.maps=8 -D mapreduce.job.reduces=8 32000 hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Terasort/Input
19/04/24 01:56:58 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 01:56:58 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 01:56:58 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/bench/.staging/job_1556063405565_0011
19/04/24 01:56:59 INFO terasort.TeraGen: Generating 32000 using 8
19/04/24 01:56:59 INFO mapreduce.JobSubmitter: number of splits:8
19/04/24 01:56:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1556063405565_0011
19/04/24 01:56:59 INFO mapreduce.JobSubmitter: Executing with tokens: []
19/04/24 01:56:59 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/04/24 01:57:00 INFO impl.YarnClientImpl: Submitted application application_1556063405565_0011
19/04/24 01:57:00 INFO mapreduce.Job: The url to track the job: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0011/
19/04/24 01:57:00 INFO mapreduce.Job: Running job: job_1556063405565_0011
19/04/24 01:57:06 INFO mapreduce.Job: Job job_1556063405565_0011 running in uber mode : false
19/04/24 01:57:06 INFO mapreduce.Job:  map 0% reduce 0%
19/04/24 01:57:10 INFO mapreduce.Job:  map 13% reduce 0%
19/04/24 01:57:11 INFO mapreduce.Job:  map 50% reduce 0%
19/04/24 01:57:12 INFO mapreduce.Job:  map 100% reduce 0%
19/04/24 01:57:12 INFO mapreduce.Job: Job job_1556063405565_0011 completed successfully
19/04/24 01:57:12 INFO mapreduce.Job: Counters: 33
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=1864728
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=662
		HDFS: Number of bytes written=3200000
		HDFS: Number of read operations=48
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=16
	Job Counters 
		Launched map tasks=8
		Other local map tasks=8
		Total time spent by all maps in occupied slots (ms)=105316
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=26329
		Total vcore-milliseconds taken by all map tasks=26329
		Total megabyte-milliseconds taken by all map tasks=107843584
	Map-Reduce Framework
		Map input records=32000
		Map output records=32000
		Input split bytes=662
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=969
		CPU time spent (ms)=6390
		Physical memory (bytes) snapshot=1807740928
		Virtual memory (bytes) snapshot=44113145856
		Total committed heap usage (bytes)=1596456960
		Peak Map Physical memory (bytes)=227508224
		Peak Map Virtual memory (bytes)=5516017664
	org.apache.hadoop.examples.terasort.TeraGen$Counters
		CHECKSUM=68613941816777
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=3200000
finish HadoopPrepareTerasort bench
Run micro/terasort/hadoop
Exec script: /home/bench/HiBench/bin/workloads/micro/terasort/hadoop/run.sh
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/micro/terasort.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start HadoopTerasort bench
hdfs rm -r: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -rm -r -skipTrash hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Terasort/Output
rm: `hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Terasort/Output': No such file or directory
hdfs du -s: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -du -s hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Terasort/Input
Submit MapReduce Job: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop jar /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-examples.jar terasort -D mapreduce.job.reduces=8 hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Terasort/Input hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Terasort/Output
19/04/24 01:57:19 INFO terasort.TeraSort: starting
19/04/24 01:57:21 INFO input.FileInputFormat: Total input files to process : 8
Spent 276ms computing base-splits.
Spent 3ms computing TeraScheduler splits.
Computing input splits took 280ms
Sampling 8 splits of 8
Making 8 from 32000 sampled records
Computing parititions took 386ms
Spent 669ms computing partitions.
19/04/24 01:57:22 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 01:57:22 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 01:57:22 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/bench/.staging/job_1556063405565_0012
19/04/24 01:57:22 INFO mapreduce.JobSubmitter: number of splits:8
19/04/24 01:57:23 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1556063405565_0012
19/04/24 01:57:23 INFO mapreduce.JobSubmitter: Executing with tokens: []
19/04/24 01:57:23 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/04/24 01:57:23 INFO impl.YarnClientImpl: Submitted application application_1556063405565_0012
19/04/24 01:57:23 INFO mapreduce.Job: The url to track the job: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0012/
19/04/24 01:57:23 INFO mapreduce.Job: Running job: job_1556063405565_0012
19/04/24 01:57:30 INFO mapreduce.Job: Job job_1556063405565_0012 running in uber mode : false
19/04/24 01:57:30 INFO mapreduce.Job:  map 0% reduce 0%
19/04/24 01:57:36 INFO mapreduce.Job:  map 25% reduce 0%
19/04/24 01:57:37 INFO mapreduce.Job:  map 50% reduce 0%
19/04/24 01:57:38 INFO mapreduce.Job:  map 63% reduce 0%
19/04/24 01:57:39 INFO mapreduce.Job:  map 100% reduce 0%
19/04/24 01:57:42 INFO mapreduce.Job:  map 100% reduce 13%
19/04/24 01:57:44 INFO mapreduce.Job:  map 100% reduce 25%
19/04/24 01:57:45 INFO mapreduce.Job:  map 100% reduce 38%
19/04/24 01:57:47 INFO mapreduce.Job:  map 100% reduce 50%
19/04/24 01:57:50 INFO mapreduce.Job:  map 100% reduce 63%
19/04/24 01:57:53 INFO mapreduce.Job:  map 100% reduce 75%
19/04/24 01:57:54 INFO mapreduce.Job:  map 100% reduce 88%
19/04/24 01:57:56 INFO mapreduce.Job:  map 100% reduce 100%
19/04/24 01:57:58 INFO mapreduce.Job: Job job_1556063405565_0012 completed successfully
19/04/24 01:57:58 INFO mapreduce.Job: Counters: 54
	File System Counters
		FILE: Number of bytes read=3328664
		FILE: Number of bytes written=10410376
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=3201120
		HDFS: Number of bytes written=3200000
		HDFS: Number of read operations=64
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=16
	Job Counters 
		Launched map tasks=8
		Launched reduce tasks=8
		Data-local map tasks=5
		Rack-local map tasks=3
		Total time spent by all maps in occupied slots (ms)=184916
		Total time spent by all reduces in occupied slots (ms)=174840
		Total time spent by all map tasks (ms)=46229
		Total time spent by all reduce tasks (ms)=21855
		Total vcore-milliseconds taken by all map tasks=46229
		Total vcore-milliseconds taken by all reduce tasks=21855
		Total megabyte-milliseconds taken by all map tasks=189353984
		Total megabyte-milliseconds taken by all reduce tasks=179036160
	Map-Reduce Framework
		Map input records=32000
		Map output records=32000
		Map output bytes=3264000
		Map output materialized bytes=3328384
		Input split bytes=1120
		Combine input records=0
		Combine output records=0
		Reduce input groups=32000
		Reduce shuffle bytes=3328384
		Reduce input records=32000
		Reduce output records=32000
		Spilled Records=64000
		Shuffled Maps =64
		Failed Shuffles=0
		Merged Map outputs=64
		GC time elapsed (ms)=1537
		CPU time spent (ms)=37690
		Physical memory (bytes) snapshot=20852391936
		Virtual memory (bytes) snapshot=116940599296
		Total committed heap usage (bytes)=20314062848
		Peak Map Physical memory (bytes)=2379665408
		Peak Map Virtual memory (bytes)=5513891840
		Peak Reduce Physical memory (bytes)=234233856
		Peak Reduce Virtual memory (bytes)=9107251200
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=3200000
	File Output Format Counters 
		Bytes Written=3200000
19/04/24 01:57:58 INFO terasort.TeraSort: done
finish HadoopTerasort bench
Run micro/terasort/spark
Exec script: /home/bench/HiBench/bin/workloads/micro/terasort/spark/run.sh
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/micro/terasort.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start ScalaSparkTerasort bench
hdfs rm -r: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -rm -r -skipTrash hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Terasort/Output
Deleted hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Terasort/Output
hdfs du -s: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -du -s hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Terasort/Input
Export env: SPARKBENCH_PROPERTIES_FILES=/home/bench/HiBench/report/terasort/spark/conf/sparkbench/sparkbench.conf
Submit Spark job: /usr/hdp/current/spark2-client/bin/spark-submit  --properties-file /home/bench/HiBench/report/terasort/spark/conf/sparkbench/spark.conf --class com.intel.hibench.sparkbench.micro.ScalaTeraSort --master yarn  /home/bench/HiBench/sparkbench/assembly/target/sparkbench-assembly-7.1-SNAPSHOT-dist.jar hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Terasort/Input hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Terasort/Output
19/04/24 01:58:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/04/24 01:58:06 INFO SparkContext: Running Spark version 2.3.2.3.1.0.0-78
19/04/24 01:58:06 INFO SparkContext: Submitted application: ScalaTeraSort
19/04/24 01:58:06 INFO SecurityManager: Changing view acls to: bench
19/04/24 01:58:06 INFO SecurityManager: Changing modify acls to: bench
19/04/24 01:58:06 INFO SecurityManager: Changing view acls groups to: 
19/04/24 01:58:06 INFO SecurityManager: Changing modify acls groups to: 
19/04/24 01:58:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bench); groups with view permissions: Set(); users  with modify permissions: Set(bench); groups with modify permissions: Set()
19/04/24 01:58:06 INFO Utils: Successfully started service 'sparkDriver' on port 33783.
19/04/24 01:58:07 INFO SparkEnv: Registering MapOutputTracker
19/04/24 01:58:07 INFO SparkEnv: Registering BlockManagerMaster
19/04/24 01:58:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/04/24 01:58:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/04/24 01:58:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-51f8d1f3-896f-437c-abce-12bf7c71c343
19/04/24 01:58:07 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB
19/04/24 01:58:07 INFO SparkEnv: Registering OutputCommitCoordinator
19/04/24 01:58:07 INFO log: Logging initialized @2648ms
19/04/24 01:58:07 INFO Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T19:11:56+02:00, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
19/04/24 01:58:07 INFO Server: Started @2785ms
19/04/24 01:58:07 INFO AbstractConnector: Started ServerConnector@7d61eccf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19/04/24 01:58:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7d373bcf{/jobs,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5c77053b{/jobs/json,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@26b894bd{/jobs/job,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5489c777{/jobs/job/json,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3676ac27{/stages,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@62f87c44{/stages/json,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@48f5bde6{/stages/stage,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7072bc39{/stages/stage/json,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@158d255c{/stages/pool,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2ca65ce4{/stages/pool/json,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@327120c8{/storage,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5707c1cb{/storage/json,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2b5cb9b2{/storage/rdd,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@35038141{/storage/rdd/json,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@ecf9049{/environment,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@672f11c2{/environment/json,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2970a5bc{/executors,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@50305a{/executors/json,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@72efb5c1{/executors/threadDump,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6d511b5f{/executors/threadDump/json,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@41200e0c{/static,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3b1bb3ab{/,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5a4bef8{/api,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@62da83ed{/jobs/job/kill,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5d8445d7{/stages/stage/kill,null,AVAILABLE,@Spark}
19/04/24 01:58:07 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://varlet1.fyre.ibm.com:4040
19/04/24 01:58:07 INFO SparkContext: Added JAR file:/home/bench/HiBench/sparkbench/assembly/target/sparkbench-assembly-7.1-SNAPSHOT-dist.jar at spark://varlet1.fyre.ibm.com:33783/jars/sparkbench-assembly-7.1-SNAPSHOT-dist.jar with timestamp 1556063887558
19/04/24 01:58:08 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
19/04/24 01:58:08 INFO RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 01:58:08 INFO Client: Requesting a new application from cluster with 5 NodeManagers
19/04/24 01:58:09 INFO Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/04/24 01:58:09 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
19/04/24 01:58:09 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
19/04/24 01:58:09 INFO Client: Setting up container launch context for our AM
19/04/24 01:58:09 INFO Client: Setting up the launch environment for our AM container
19/04/24 01:58:09 INFO Client: Preparing resources for our AM container
19/04/24 01:58:10 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://a1.fyre.ibm.com:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-yarn-archive.tar.gz
19/04/24 01:58:10 INFO Client: Source and destination file systems are the same. Not copying hdfs://a1.fyre.ibm.com:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-yarn-archive.tar.gz
19/04/24 01:58:10 INFO Client: Distribute hdfs cache file as spark.sql.hive.metastore.jars for HDP, hdfsCacheFile:hdfs://a1.fyre.ibm.com:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-hive-archive.tar.gz
19/04/24 01:58:10 INFO Client: Source and destination file systems are the same. Not copying hdfs://a1.fyre.ibm.com:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-hive-archive.tar.gz
19/04/24 01:58:11 INFO Client: Uploading resource file:/tmp/spark-6013c93d-3ad6-41fa-b2e9-d595111b65c9/__spark_conf__3706658034289643474.zip -> hdfs://a1.fyre.ibm.com:8020/user/bench/.sparkStaging/application_1556063405565_0013/__spark_conf__.zip
19/04/24 01:58:11 INFO SecurityManager: Changing view acls to: bench
19/04/24 01:58:11 INFO SecurityManager: Changing modify acls to: bench
19/04/24 01:58:11 INFO SecurityManager: Changing view acls groups to: 
19/04/24 01:58:11 INFO SecurityManager: Changing modify acls groups to: 
19/04/24 01:58:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bench); groups with view permissions: Set(); users  with modify permissions: Set(bench); groups with modify permissions: Set()
19/04/24 01:58:11 INFO Client: Submitting application application_1556063405565_0013 to ResourceManager
19/04/24 01:58:11 INFO YarnClientImpl: Submitted application application_1556063405565_0013
19/04/24 01:58:11 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1556063405565_0013 and attemptId None
19/04/24 01:58:12 INFO Client: Application report for application_1556063405565_0013 (state: ACCEPTED)
19/04/24 01:58:12 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1556063891508
	 final status: UNDEFINED
	 tracking URL: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0013/
	 user: bench
19/04/24 01:58:13 INFO Client: Application report for application_1556063405565_0013 (state: ACCEPTED)
19/04/24 01:58:14 INFO Client: Application report for application_1556063405565_0013 (state: ACCEPTED)
19/04/24 01:58:15 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> a1.fyre.ibm.com, PROXY_URI_BASES -> http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0013), /proxy/application_1556063405565_0013
19/04/24 01:58:15 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.
19/04/24 01:58:15 INFO Client: Application report for application_1556063405565_0013 (state: RUNNING)
19/04/24 01:58:15 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.16.191.130
	 ApplicationMaster RPC port: 0
	 queue: default
	 start time: 1556063891508
	 final status: UNDEFINED
	 tracking URL: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0013/
	 user: bench
19/04/24 01:58:15 INFO YarnClientSchedulerBackend: Application application_1556063405565_0013 has started running.
19/04/24 01:58:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44293.
19/04/24 01:58:15 INFO NettyBlockTransferService: Server created on varlet1.fyre.ibm.com:44293
19/04/24 01:58:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/04/24 01:58:15 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
19/04/24 01:58:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, varlet1.fyre.ibm.com, 44293, None)
19/04/24 01:58:15 INFO BlockManagerMasterEndpoint: Registering block manager varlet1.fyre.ibm.com:44293 with 2004.6 MB RAM, BlockManagerId(driver, varlet1.fyre.ibm.com, 44293, None)
19/04/24 01:58:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, varlet1.fyre.ibm.com, 44293, None)
19/04/24 01:58:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, varlet1.fyre.ibm.com, 44293, None)
19/04/24 01:58:16 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.
19/04/24 01:58:16 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@15605d83{/metrics/json,null,AVAILABLE,@Spark}
19/04/24 01:58:19 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.191.118:46728) with ID 2
19/04/24 01:58:19 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.191.17:34424) with ID 1
19/04/24 01:58:19 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
19/04/24 01:58:19 INFO BlockManagerMasterEndpoint: Registering block manager hurds5.fyre.ibm.com:36169 with 2004.6 MB RAM, BlockManagerId(2, hurds5.fyre.ibm.com, 36169, None)
19/04/24 01:58:19 INFO BlockManagerMasterEndpoint: Registering block manager hurds2.fyre.ibm.com:40865 with 2004.6 MB RAM, BlockManagerId(1, hurds2.fyre.ibm.com, 40865, None)
19/04/24 01:58:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 367.4 KB, free 2004.2 MB)
19/04/24 01:58:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KB, free 2004.2 MB)
19/04/24 01:58:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on varlet1.fyre.ibm.com:44293 (size: 32.5 KB, free: 2004.6 MB)
19/04/24 01:58:20 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ScalaTeraSort.scala:49
19/04/24 01:58:20 INFO FileInputFormat: Total input files to process : 8
Spent 67ms computing base-splits.
Spent 5ms computing TeraScheduler splits.
19/04/24 01:58:20 INFO SparkContext: Starting job: BaseRangePartitioner at ScalaTeraSort.scala:56
19/04/24 01:58:20 INFO DAGScheduler: Got job 0 (BaseRangePartitioner at ScalaTeraSort.scala:56) with 8 output partitions
19/04/24 01:58:20 INFO DAGScheduler: Final stage: ResultStage 0 (BaseRangePartitioner at ScalaTeraSort.scala:56)
19/04/24 01:58:20 INFO DAGScheduler: Parents of final stage: List()
19/04/24 01:58:20 INFO DAGScheduler: Missing parents: List()
19/04/24 01:58:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at BaseRangePartitioner at ScalaTeraSort.scala:56), which has no missing parents
19/04/24 01:58:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.6 KB, free 2004.2 MB)
19/04/24 01:58:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 2004.2 MB)
19/04/24 01:58:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on varlet1.fyre.ibm.com:44293 (size: 2.1 KB, free: 2004.6 MB)
19/04/24 01:58:20 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1039
19/04/24 01:58:20 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at BaseRangePartitioner at ScalaTeraSort.scala:56) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
19/04/24 01:58:20 INFO YarnScheduler: Adding task set 0.0 with 8 tasks
19/04/24 01:58:20 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 0, hurds2.fyre.ibm.com, executor 1, partition 4, NODE_LOCAL, 8072 bytes)
19/04/24 01:58:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hurds2.fyre.ibm.com:40865 (size: 2.1 KB, free: 2004.6 MB)
19/04/24 01:58:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hurds2.fyre.ibm.com:40865 (size: 32.5 KB, free: 2004.6 MB)
19/04/24 01:58:22 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 1, hurds2.fyre.ibm.com, executor 1, partition 6, NODE_LOCAL, 8072 bytes)
19/04/24 01:58:22 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 0) in 2394 ms on hurds2.fyre.ibm.com (executor 1) (1/8)
19/04/24 01:58:22 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 1) in 76 ms on hurds2.fyre.ibm.com (executor 1) (2/8)
19/04/24 01:58:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 2, hurds5.fyre.ibm.com, executor 2, partition 0, RACK_LOCAL, 8072 bytes)
19/04/24 01:58:26 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 3, hurds2.fyre.ibm.com, executor 1, partition 1, RACK_LOCAL, 8072 bytes)
19/04/24 01:58:26 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 4, hurds2.fyre.ibm.com, executor 1, partition 2, RACK_LOCAL, 8072 bytes)
19/04/24 01:58:26 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 3) in 42 ms on hurds2.fyre.ibm.com (executor 1) (3/8)
19/04/24 01:58:26 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 5, hurds2.fyre.ibm.com, executor 1, partition 3, RACK_LOCAL, 8072 bytes)
19/04/24 01:58:26 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 4) in 44 ms on hurds2.fyre.ibm.com (executor 1) (4/8)
19/04/24 01:58:26 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 6, hurds2.fyre.ibm.com, executor 1, partition 5, RACK_LOCAL, 8072 bytes)
19/04/24 01:58:26 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 5) in 28 ms on hurds2.fyre.ibm.com (executor 1) (5/8)
19/04/24 01:58:26 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, hurds2.fyre.ibm.com, executor 1, partition 7, RACK_LOCAL, 8072 bytes)
19/04/24 01:58:26 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 6) in 40 ms on hurds2.fyre.ibm.com (executor 1) (6/8)
19/04/24 01:58:26 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 49 ms on hurds2.fyre.ibm.com (executor 1) (7/8)
19/04/24 01:58:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hurds5.fyre.ibm.com:36169 (size: 2.1 KB, free: 2004.6 MB)
19/04/24 01:58:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hurds5.fyre.ibm.com:36169 (size: 32.5 KB, free: 2004.6 MB)
19/04/24 01:58:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 2) in 2387 ms on hurds5.fyre.ibm.com (executor 2) (8/8)
19/04/24 01:58:29 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
19/04/24 01:58:29 INFO DAGScheduler: ResultStage 0 (BaseRangePartitioner at ScalaTeraSort.scala:56) finished in 8,833 s
19/04/24 01:58:29 INFO DAGScheduler: Job 0 finished: BaseRangePartitioner at ScalaTeraSort.scala:56, took 8,921032 s
19/04/24 01:58:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
19/04/24 01:58:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
19/04/24 01:58:29 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78
19/04/24 01:58:29 INFO DAGScheduler: Registering RDD 1 (map at ScalaTeraSort.scala:49)
19/04/24 01:58:29 INFO DAGScheduler: Got job 1 (runJob at SparkHadoopWriter.scala:78) with 8 output partitions
19/04/24 01:58:29 INFO DAGScheduler: Final stage: ResultStage 2 (runJob at SparkHadoopWriter.scala:78)
19/04/24 01:58:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
19/04/24 01:58:29 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
19/04/24 01:58:29 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[1] at map at ScalaTeraSort.scala:49), which has no missing parents
19/04/24 01:58:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 2004.2 MB)
19/04/24 01:58:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 2004.2 MB)
19/04/24 01:58:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on varlet1.fyre.ibm.com:44293 (size: 2.7 KB, free: 2004.6 MB)
19/04/24 01:58:29 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1039
19/04/24 01:58:29 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[1] at map at ScalaTeraSort.scala:49) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
19/04/24 01:58:29 INFO YarnScheduler: Adding task set 1.0 with 8 tasks
19/04/24 01:58:29 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 8, hurds2.fyre.ibm.com, executor 1, partition 4, NODE_LOCAL, 7964 bytes)
19/04/24 01:58:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hurds2.fyre.ibm.com:40865 (size: 2.7 KB, free: 2004.6 MB)
19/04/24 01:58:29 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 9, hurds2.fyre.ibm.com, executor 1, partition 6, NODE_LOCAL, 7964 bytes)
19/04/24 01:58:29 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 8) in 430 ms on hurds2.fyre.ibm.com (executor 1) (1/8)
19/04/24 01:58:29 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 9) in 85 ms on hurds2.fyre.ibm.com (executor 1) (2/8)
19/04/24 01:58:32 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 10, hurds2.fyre.ibm.com, executor 1, partition 0, RACK_LOCAL, 7964 bytes)
19/04/24 01:58:32 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 11, hurds5.fyre.ibm.com, executor 2, partition 1, RACK_LOCAL, 7964 bytes)
19/04/24 01:58:32 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hurds5.fyre.ibm.com:36169 (size: 2.7 KB, free: 2004.6 MB)
19/04/24 01:58:32 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 12, hurds2.fyre.ibm.com, executor 1, partition 2, RACK_LOCAL, 7964 bytes)
19/04/24 01:58:32 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 10) in 183 ms on hurds2.fyre.ibm.com (executor 1) (3/8)
19/04/24 01:58:33 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 13, hurds2.fyre.ibm.com, executor 1, partition 3, RACK_LOCAL, 7964 bytes)
19/04/24 01:58:33 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 12) in 63 ms on hurds2.fyre.ibm.com (executor 1) (4/8)
19/04/24 01:58:33 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 14, hurds2.fyre.ibm.com, executor 1, partition 5, RACK_LOCAL, 7964 bytes)
19/04/24 01:58:33 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 13) in 58 ms on hurds2.fyre.ibm.com (executor 1) (5/8)
19/04/24 01:58:33 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 15, hurds2.fyre.ibm.com, executor 1, partition 7, RACK_LOCAL, 7964 bytes)
19/04/24 01:58:33 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 14) in 56 ms on hurds2.fyre.ibm.com (executor 1) (6/8)
19/04/24 01:58:33 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 11) in 390 ms on hurds5.fyre.ibm.com (executor 2) (7/8)
19/04/24 01:58:33 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 15) in 55 ms on hurds2.fyre.ibm.com (executor 1) (8/8)
19/04/24 01:58:33 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
19/04/24 01:58:33 INFO DAGScheduler: ShuffleMapStage 1 (map at ScalaTeraSort.scala:49) finished in 3,862 s
19/04/24 01:58:33 INFO DAGScheduler: looking for newly runnable stages
19/04/24 01:58:33 INFO DAGScheduler: running: Set()
19/04/24 01:58:33 INFO DAGScheduler: waiting: Set(ResultStage 2)
19/04/24 01:58:33 INFO DAGScheduler: failed: Set()
19/04/24 01:58:33 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at map at ScalaTeraSort.scala:58), which has no missing parents
19/04/24 01:58:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 94.7 KB, free 2004.1 MB)
19/04/24 01:58:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 35.5 KB, free 2004.1 MB)
19/04/24 01:58:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on varlet1.fyre.ibm.com:44293 (size: 35.5 KB, free: 2004.5 MB)
19/04/24 01:58:33 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1039
19/04/24 01:58:33 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at map at ScalaTeraSort.scala:58) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
19/04/24 01:58:33 INFO YarnScheduler: Adding task set 2.0 with 8 tasks
19/04/24 01:58:33 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 16, hurds2.fyre.ibm.com, executor 1, partition 0, NODE_LOCAL, 7660 bytes)
19/04/24 01:58:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hurds2.fyre.ibm.com:40865 (size: 35.5 KB, free: 2004.5 MB)
19/04/24 01:58:33 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.16.191.17:34424
19/04/24 01:58:33 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 17, hurds2.fyre.ibm.com, executor 1, partition 1, NODE_LOCAL, 7660 bytes)
19/04/24 01:58:33 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 16) in 561 ms on hurds2.fyre.ibm.com (executor 1) (1/8)
19/04/24 01:58:33 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 18, hurds2.fyre.ibm.com, executor 1, partition 2, NODE_LOCAL, 7660 bytes)
19/04/24 01:58:33 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 17) in 156 ms on hurds2.fyre.ibm.com (executor 1) (2/8)
19/04/24 01:58:34 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 19, hurds2.fyre.ibm.com, executor 1, partition 3, NODE_LOCAL, 7660 bytes)
19/04/24 01:58:34 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 18) in 164 ms on hurds2.fyre.ibm.com (executor 1) (3/8)
19/04/24 01:58:34 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 20, hurds2.fyre.ibm.com, executor 1, partition 4, NODE_LOCAL, 7660 bytes)
19/04/24 01:58:34 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 19) in 135 ms on hurds2.fyre.ibm.com (executor 1) (4/8)
19/04/24 01:58:34 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 21, hurds2.fyre.ibm.com, executor 1, partition 5, NODE_LOCAL, 7660 bytes)
19/04/24 01:58:34 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 20) in 157 ms on hurds2.fyre.ibm.com (executor 1) (5/8)
19/04/24 01:58:34 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 22, hurds2.fyre.ibm.com, executor 1, partition 6, NODE_LOCAL, 7660 bytes)
19/04/24 01:58:34 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 21) in 152 ms on hurds2.fyre.ibm.com (executor 1) (6/8)
19/04/24 01:58:34 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 23, hurds2.fyre.ibm.com, executor 1, partition 7, NODE_LOCAL, 7660 bytes)
19/04/24 01:58:34 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 22) in 161 ms on hurds2.fyre.ibm.com (executor 1) (7/8)
19/04/24 01:58:34 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 23) in 156 ms on hurds2.fyre.ibm.com (executor 1) (8/8)
19/04/24 01:58:34 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
19/04/24 01:58:34 INFO DAGScheduler: ResultStage 2 (runJob at SparkHadoopWriter.scala:78) finished in 1,662 s
19/04/24 01:58:34 INFO DAGScheduler: Job 1 finished: runJob at SparkHadoopWriter.scala:78, took 5,563188 s
19/04/24 01:58:34 INFO SparkHadoopWriter: Job job_20190424015829_0006 committed.
19/04/24 01:58:34 INFO AbstractConnector: Stopped Spark@7d61eccf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19/04/24 01:58:34 INFO SparkUI: Stopped Spark web UI at http://varlet1.fyre.ibm.com:4040
19/04/24 01:58:34 INFO YarnClientSchedulerBackend: Interrupting monitor thread
19/04/24 01:58:34 INFO YarnClientSchedulerBackend: Shutting down all executors
19/04/24 01:58:34 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
19/04/24 01:58:34 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
19/04/24 01:58:34 INFO YarnClientSchedulerBackend: Stopped
19/04/24 01:58:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/04/24 01:58:35 INFO MemoryStore: MemoryStore cleared
19/04/24 01:58:35 INFO BlockManager: BlockManager stopped
19/04/24 01:58:35 INFO BlockManagerMaster: BlockManagerMaster stopped
19/04/24 01:58:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/04/24 01:58:35 INFO SparkContext: Successfully stopped SparkContext
19/04/24 01:58:35 INFO ShutdownHookManager: Shutdown hook called
19/04/24 01:58:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-f0915cc4-ea2d-4016-bc9e-10dcf05dab0e
19/04/24 01:58:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-6013c93d-3ad6-41fa-b2e9-d595111b65c9
finish ScalaSparkTerasort bench
Prepare micro.wordcount ...
Exec script: /home/bench/HiBench/bin/workloads/micro/wordcount/prepare/prepare.sh
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/micro/wordcount.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start HadoopPrepareWordcount bench
hdfs rm -r: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -rm -r -skipTrash hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Wordcount/Input
Deleted hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Wordcount/Input
Submit MapReduce Job: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop jar /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-examples.jar randomtextwriter -D mapreduce.randomtextwriter.totalbytes=32000 -D mapreduce.randomtextwriter.bytespermap=4000 -D mapreduce.job.maps=8 -D mapreduce.job.reduces=8 hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Wordcount/Input
19/04/24 01:58:40 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 01:58:40 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
Running 8 maps.
Job started: Wed Apr 24 01:58:41 CEST 2019
19/04/24 01:58:41 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 01:58:41 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 01:58:42 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/bench/.staging/job_1556063405565_0014
19/04/24 01:58:42 INFO mapreduce.JobSubmitter: number of splits:8
19/04/24 01:58:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1556063405565_0014
19/04/24 01:58:42 INFO mapreduce.JobSubmitter: Executing with tokens: []
19/04/24 01:58:43 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/04/24 01:58:43 INFO impl.YarnClientImpl: Submitted application application_1556063405565_0014
19/04/24 01:58:43 INFO mapreduce.Job: The url to track the job: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0014/
19/04/24 01:58:43 INFO mapreduce.Job: Running job: job_1556063405565_0014
19/04/24 01:58:49 INFO mapreduce.Job: Job job_1556063405565_0014 running in uber mode : false
19/04/24 01:58:49 INFO mapreduce.Job:  map 0% reduce 0%
19/04/24 01:58:53 INFO mapreduce.Job:  map 13% reduce 0%
19/04/24 01:58:54 INFO mapreduce.Job:  map 100% reduce 0%
19/04/24 01:58:55 INFO mapreduce.Job: Job job_1556063405565_0014 completed successfully
19/04/24 01:58:55 INFO mapreduce.Job: Counters: 34
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=1859224
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1136
		HDFS: Number of bytes written=36956
		HDFS: Number of read operations=48
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=16
	Job Counters 
		Launched map tasks=8
		Other local map tasks=8
		Total time spent by all maps in occupied slots (ms)=97704
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=24426
		Total vcore-milliseconds taken by all map tasks=24426
		Total megabyte-milliseconds taken by all map tasks=100048896
	Map-Reduce Framework
		Map input records=8
		Map output records=52
		Input split bytes=1136
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=943
		CPU time spent (ms)=5320
		Physical memory (bytes) snapshot=1823375360
		Virtual memory (bytes) snapshot=44108189696
		Total committed heap usage (bytes)=1592786944
		Peak Map Physical memory (bytes)=233799680
		Peak Map Virtual memory (bytes)=5515276288
	org.apache.hadoop.examples.RandomTextWriter$Counters
		BYTES_WRITTEN=35718
		RECORDS_WRITTEN=52
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=36956
Job ended: Wed Apr 24 01:58:55 CEST 2019
The job took 14 seconds.
finish HadoopPrepareWordcount bench
Run micro/wordcount/hadoop
Exec script: /home/bench/HiBench/bin/workloads/micro/wordcount/hadoop/run.sh
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/micro/wordcount.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start HadoopWordcount bench
hdfs rm -r: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -rm -r -skipTrash hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Wordcount/Output
Deleted hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Wordcount/Output
hdfs du -s: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -du -s hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Wordcount/Input
Submit MapReduce Job: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop jar /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount -D mapreduce.job.maps=8 -D mapreduce.job.reduces=8 -D mapreduce.inputformat.class=org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat -D mapreduce.outputformat.class=org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat -D mapreduce.job.inputformat.class=org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat -D mapreduce.job.outputformat.class=org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Wordcount/Input hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Wordcount/Output
19/04/24 01:59:04 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 01:59:04 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 01:59:04 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/bench/.staging/job_1556063405565_0015
19/04/24 01:59:05 INFO input.FileInputFormat: Total input files to process : 8
19/04/24 01:59:05 INFO mapreduce.JobSubmitter: number of splits:8
19/04/24 01:59:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1556063405565_0015
19/04/24 01:59:05 INFO mapreduce.JobSubmitter: Executing with tokens: []
19/04/24 01:59:06 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/04/24 01:59:06 INFO impl.YarnClientImpl: Submitted application application_1556063405565_0015
19/04/24 01:59:06 INFO mapreduce.Job: The url to track the job: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0015/
19/04/24 01:59:06 INFO mapreduce.Job: Running job: job_1556063405565_0015
19/04/24 01:59:12 INFO mapreduce.Job: Job job_1556063405565_0015 running in uber mode : false
19/04/24 01:59:12 INFO mapreduce.Job:  map 0% reduce 0%
19/04/24 01:59:18 INFO mapreduce.Job:  map 13% reduce 0%
19/04/24 01:59:19 INFO mapreduce.Job:  map 38% reduce 0%
19/04/24 01:59:20 INFO mapreduce.Job:  map 75% reduce 0%
19/04/24 01:59:21 INFO mapreduce.Job:  map 100% reduce 0%
19/04/24 01:59:23 INFO mapreduce.Job:  map 100% reduce 13%
19/04/24 01:59:24 INFO mapreduce.Job:  map 100% reduce 38%
19/04/24 01:59:27 INFO mapreduce.Job:  map 100% reduce 50%
19/04/24 01:59:28 INFO mapreduce.Job:  map 100% reduce 75%
19/04/24 01:59:31 INFO mapreduce.Job:  map 100% reduce 88%
19/04/24 01:59:32 INFO mapreduce.Job:  map 100% reduce 100%
19/04/24 01:59:32 INFO mapreduce.Job: Job job_1556063405565_0015 completed successfully
19/04/24 01:59:32 INFO mapreduce.Job: Counters: 54
	File System Counters
		FILE: Number of bytes read=41344
		FILE: Number of bytes written=3827240
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=38084
		HDFS: Number of bytes written=22186
		HDFS: Number of read operations=72
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=16
	Job Counters 
		Launched map tasks=8
		Launched reduce tasks=8
		Data-local map tasks=6
		Rack-local map tasks=2
		Total time spent by all maps in occupied slots (ms)=192844
		Total time spent by all reduces in occupied slots (ms)=176008
		Total time spent by all map tasks (ms)=48211
		Total time spent by all reduce tasks (ms)=22001
		Total vcore-milliseconds taken by all map tasks=48211
		Total vcore-milliseconds taken by all reduce tasks=22001
		Total megabyte-milliseconds taken by all map tasks=197472256
		Total megabyte-milliseconds taken by all reduce tasks=180232192
	Map-Reduce Framework
		Map input records=52
		Map output records=2973
		Map output bytes=43889
		Map output materialized bytes=41680
		Input split bytes=1128
		Combine input records=2973
		Combine output records=2467
		Reduce input groups=947
		Reduce shuffle bytes=41680
		Reduce input records=2467
		Reduce output records=947
		Spilled Records=4934
		Shuffled Maps =64
		Failed Shuffles=0
		Merged Map outputs=64
		GC time elapsed (ms)=1895
		CPU time spent (ms)=33840
		Physical memory (bytes) snapshot=20878376960
		Virtual memory (bytes) snapshot=116951613440
		Total committed heap usage (bytes)=20275265536
		Peak Map Physical memory (bytes)=2379968512
		Peak Map Virtual memory (bytes)=5512622080
		Peak Reduce Physical memory (bytes)=236630016
		Peak Reduce Virtual memory (bytes)=9109495808
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=36956
	File Output Format Counters 
		Bytes Written=22186
finish HadoopWordcount bench
Run micro/wordcount/spark
Exec script: /home/bench/HiBench/bin/workloads/micro/wordcount/spark/run.sh
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/micro/wordcount.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start ScalaSparkWordcount bench
hdfs rm -r: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -rm -r -skipTrash hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Wordcount/Output
Deleted hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Wordcount/Output
hdfs du -s: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -du -s hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Wordcount/Input
Export env: SPARKBENCH_PROPERTIES_FILES=/home/bench/HiBench/report/wordcount/spark/conf/sparkbench/sparkbench.conf
Submit Spark job: /usr/hdp/current/spark2-client/bin/spark-submit  --properties-file /home/bench/HiBench/report/wordcount/spark/conf/sparkbench/spark.conf --class com.intel.hibench.sparkbench.micro.ScalaWordCount --master yarn  /home/bench/HiBench/sparkbench/assembly/target/sparkbench-assembly-7.1-SNAPSHOT-dist.jar hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Wordcount/Input hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Wordcount/Output
19/04/24 01:59:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/04/24 01:59:41 INFO SparkContext: Running Spark version 2.3.2.3.1.0.0-78
19/04/24 01:59:41 INFO SparkContext: Submitted application: ScalaWordCount
19/04/24 01:59:41 INFO SecurityManager: Changing view acls to: bench
19/04/24 01:59:41 INFO SecurityManager: Changing modify acls to: bench
19/04/24 01:59:41 INFO SecurityManager: Changing view acls groups to: 
19/04/24 01:59:41 INFO SecurityManager: Changing modify acls groups to: 
19/04/24 01:59:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bench); groups with view permissions: Set(); users  with modify permissions: Set(bench); groups with modify permissions: Set()
19/04/24 01:59:41 INFO Utils: Successfully started service 'sparkDriver' on port 44733.
19/04/24 01:59:41 INFO SparkEnv: Registering MapOutputTracker
19/04/24 01:59:41 INFO SparkEnv: Registering BlockManagerMaster
19/04/24 01:59:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/04/24 01:59:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/04/24 01:59:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-51c894c2-71f4-4806-a98c-2e034b1f9ee0
19/04/24 01:59:41 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB
19/04/24 01:59:41 INFO SparkEnv: Registering OutputCommitCoordinator
19/04/24 01:59:41 INFO log: Logging initialized @2496ms
19/04/24 01:59:41 INFO Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T19:11:56+02:00, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
19/04/24 01:59:41 INFO Server: Started @2614ms
19/04/24 01:59:42 INFO AbstractConnector: Started ServerConnector@cc6460c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19/04/24 01:59:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6d6bc158{/jobs,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@26b894bd{/jobs/json,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@287f94b1{/jobs/job,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3676ac27{/jobs/job/json,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@62f87c44{/stages,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@48f5bde6{/stages/json,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@525d79f0{/stages/stage,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@158d255c{/stages/stage/json,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2ca65ce4{/stages/pool,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@327120c8{/stages/pool/json,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5707c1cb{/storage,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2b5cb9b2{/storage/json,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@35038141{/storage/rdd,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@ecf9049{/storage/rdd/json,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@672f11c2{/environment,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2970a5bc{/environment/json,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@50305a{/executors,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@72efb5c1{/executors/json,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6d511b5f{/executors/threadDump,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@41200e0c{/executors/threadDump/json,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@40f33492{/static,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5a4bef8{/,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@40bffbca{/api,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5d8445d7{/jobs/job/kill,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@37d80fe7{/stages/stage/kill,null,AVAILABLE,@Spark}
19/04/24 01:59:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://varlet1.fyre.ibm.com:4040
19/04/24 01:59:42 INFO SparkContext: Added JAR file:/home/bench/HiBench/sparkbench/assembly/target/sparkbench-assembly-7.1-SNAPSHOT-dist.jar at spark://varlet1.fyre.ibm.com:44733/jars/sparkbench-assembly-7.1-SNAPSHOT-dist.jar with timestamp 1556063982117
19/04/24 01:59:43 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
19/04/24 01:59:43 INFO RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 01:59:43 INFO Client: Requesting a new application from cluster with 5 NodeManagers
19/04/24 01:59:43 INFO Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/04/24 01:59:43 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
19/04/24 01:59:43 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
19/04/24 01:59:43 INFO Client: Setting up container launch context for our AM
19/04/24 01:59:43 INFO Client: Setting up the launch environment for our AM container
19/04/24 01:59:43 INFO Client: Preparing resources for our AM container
19/04/24 01:59:45 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://a1.fyre.ibm.com:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-yarn-archive.tar.gz
19/04/24 01:59:45 INFO Client: Source and destination file systems are the same. Not copying hdfs://a1.fyre.ibm.com:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-yarn-archive.tar.gz
19/04/24 01:59:45 INFO Client: Distribute hdfs cache file as spark.sql.hive.metastore.jars for HDP, hdfsCacheFile:hdfs://a1.fyre.ibm.com:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-hive-archive.tar.gz
19/04/24 01:59:45 INFO Client: Source and destination file systems are the same. Not copying hdfs://a1.fyre.ibm.com:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-hive-archive.tar.gz
19/04/24 01:59:45 INFO Client: Uploading resource file:/tmp/spark-aee05420-0e7d-4ea9-b738-c2c509945417/__spark_conf__522489834931494880.zip -> hdfs://a1.fyre.ibm.com:8020/user/bench/.sparkStaging/application_1556063405565_0016/__spark_conf__.zip
19/04/24 01:59:45 INFO SecurityManager: Changing view acls to: bench
19/04/24 01:59:45 INFO SecurityManager: Changing modify acls to: bench
19/04/24 01:59:45 INFO SecurityManager: Changing view acls groups to: 
19/04/24 01:59:45 INFO SecurityManager: Changing modify acls groups to: 
19/04/24 01:59:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bench); groups with view permissions: Set(); users  with modify permissions: Set(bench); groups with modify permissions: Set()
19/04/24 01:59:45 INFO Client: Submitting application application_1556063405565_0016 to ResourceManager
19/04/24 01:59:46 INFO YarnClientImpl: Submitted application application_1556063405565_0016
19/04/24 01:59:46 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1556063405565_0016 and attemptId None
19/04/24 01:59:47 INFO Client: Application report for application_1556063405565_0016 (state: ACCEPTED)
19/04/24 01:59:47 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1556063985919
	 final status: UNDEFINED
	 tracking URL: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0016/
	 user: bench
19/04/24 01:59:48 INFO Client: Application report for application_1556063405565_0016 (state: ACCEPTED)
19/04/24 01:59:49 INFO Client: Application report for application_1556063405565_0016 (state: ACCEPTED)
19/04/24 01:59:49 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> a1.fyre.ibm.com, PROXY_URI_BASES -> http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0016), /proxy/application_1556063405565_0016
19/04/24 01:59:49 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.
19/04/24 01:59:49 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
19/04/24 01:59:50 INFO Client: Application report for application_1556063405565_0016 (state: RUNNING)
19/04/24 01:59:50 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.16.191.17
	 ApplicationMaster RPC port: 0
	 queue: default
	 start time: 1556063985919
	 final status: UNDEFINED
	 tracking URL: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0016/
	 user: bench
19/04/24 01:59:50 INFO YarnClientSchedulerBackend: Application application_1556063405565_0016 has started running.
19/04/24 01:59:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39491.
19/04/24 01:59:50 INFO NettyBlockTransferService: Server created on varlet1.fyre.ibm.com:39491
19/04/24 01:59:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/04/24 01:59:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, varlet1.fyre.ibm.com, 39491, None)
19/04/24 01:59:50 INFO BlockManagerMasterEndpoint: Registering block manager varlet1.fyre.ibm.com:39491 with 2004.6 MB RAM, BlockManagerId(driver, varlet1.fyre.ibm.com, 39491, None)
19/04/24 01:59:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, varlet1.fyre.ibm.com, 39491, None)
19/04/24 01:59:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, varlet1.fyre.ibm.com, 39491, None)
19/04/24 01:59:50 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.
19/04/24 01:59:50 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@492521c4{/metrics/json,null,AVAILABLE,@Spark}
19/04/24 01:59:52 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.191.74:59668) with ID 1
19/04/24 01:59:52 INFO BlockManagerMasterEndpoint: Registering block manager hurds3.fyre.ibm.com:36857 with 2004.6 MB RAM, BlockManagerId(1, hurds3.fyre.ibm.com, 36857, None)
19/04/24 01:59:52 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.191.17:39118) with ID 2
19/04/24 01:59:52 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
19/04/24 01:59:52 INFO BlockManagerMasterEndpoint: Registering block manager hurds2.fyre.ibm.com:44282 with 2004.6 MB RAM, BlockManagerId(2, hurds2.fyre.ibm.com, 44282, None)
19/04/24 01:59:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 353.3 KB, free 2004.3 MB)
19/04/24 01:59:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KB, free 2004.2 MB)
19/04/24 01:59:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on varlet1.fyre.ibm.com:39491 (size: 32.5 KB, free: 2004.6 MB)
19/04/24 01:59:53 INFO SparkContext: Created broadcast 0 from sequenceFile at IOCommon.scala:44
19/04/24 01:59:53 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
19/04/24 01:59:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
19/04/24 01:59:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
19/04/24 01:59:53 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78
19/04/24 01:59:53 INFO FileInputFormat: Total input files to process : 8
19/04/24 01:59:54 INFO DAGScheduler: Registering RDD 4 (map at ScalaWordCount.scala:40)
19/04/24 01:59:54 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:78) with 8 output partitions
19/04/24 01:59:54 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:78)
19/04/24 01:59:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
19/04/24 01:59:54 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
19/04/24 01:59:54 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at map at ScalaWordCount.scala:40), which has no missing parents
19/04/24 01:59:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.5 KB, free 2004.2 MB)
19/04/24 01:59:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.1 KB, free 2004.2 MB)
19/04/24 01:59:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on varlet1.fyre.ibm.com:39491 (size: 3.1 KB, free: 2004.6 MB)
19/04/24 01:59:54 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1039
19/04/24 01:59:54 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at map at ScalaWordCount.scala:40) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
19/04/24 01:59:54 INFO YarnScheduler: Adding task set 0.0 with 8 tasks
19/04/24 01:59:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hurds2.fyre.ibm.com, executor 2, partition 0, NODE_LOCAL, 7923 bytes)
19/04/24 01:59:54 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hurds3.fyre.ibm.com, executor 1, partition 1, NODE_LOCAL, 7923 bytes)
19/04/24 01:59:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hurds3.fyre.ibm.com:36857 (size: 3.1 KB, free: 2004.6 MB)
19/04/24 01:59:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hurds2.fyre.ibm.com:44282 (size: 3.1 KB, free: 2004.6 MB)
19/04/24 01:59:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hurds3.fyre.ibm.com:36857 (size: 32.5 KB, free: 2004.6 MB)
19/04/24 01:59:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hurds2.fyre.ibm.com:44282 (size: 32.5 KB, free: 2004.6 MB)
19/04/24 01:59:56 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hurds3.fyre.ibm.com, executor 1, partition 2, NODE_LOCAL, 7923 bytes)
19/04/24 01:59:56 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2179 ms on hurds3.fyre.ibm.com (executor 1) (1/8)
19/04/24 01:59:56 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 3, hurds3.fyre.ibm.com, executor 1, partition 4, NODE_LOCAL, 7923 bytes)
19/04/24 01:59:56 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 73 ms on hurds3.fyre.ibm.com (executor 1) (2/8)
19/04/24 01:59:56 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 4, hurds3.fyre.ibm.com, executor 1, partition 5, NODE_LOCAL, 7923 bytes)
19/04/24 01:59:56 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 3) in 42 ms on hurds3.fyre.ibm.com (executor 1) (3/8)
19/04/24 01:59:56 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 5, hurds3.fyre.ibm.com, executor 1, partition 7, NODE_LOCAL, 7923 bytes)
19/04/24 01:59:56 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 4) in 37 ms on hurds3.fyre.ibm.com (executor 1) (4/8)
19/04/24 01:59:56 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 5) in 41 ms on hurds3.fyre.ibm.com (executor 1) (5/8)
19/04/24 01:59:57 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 6, hurds2.fyre.ibm.com, executor 2, partition 3, NODE_LOCAL, 7923 bytes)
19/04/24 01:59:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2617 ms on hurds2.fyre.ibm.com (executor 2) (6/8)
19/04/24 01:59:57 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 7, hurds2.fyre.ibm.com, executor 2, partition 6, NODE_LOCAL, 7923 bytes)
19/04/24 01:59:57 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 6) in 77 ms on hurds2.fyre.ibm.com (executor 2) (7/8)
19/04/24 01:59:57 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 7) in 68 ms on hurds2.fyre.ibm.com (executor 2) (8/8)
19/04/24 01:59:57 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
19/04/24 01:59:57 INFO DAGScheduler: ShuffleMapStage 0 (map at ScalaWordCount.scala:40) finished in 2,982 s
19/04/24 01:59:57 INFO DAGScheduler: looking for newly runnable stages
19/04/24 01:59:57 INFO DAGScheduler: running: Set()
19/04/24 01:59:57 INFO DAGScheduler: waiting: Set(ResultStage 1)
19/04/24 01:59:57 INFO DAGScheduler: failed: Set()
19/04/24 01:59:57 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at map at IOCommon.scala:61), which has no missing parents
19/04/24 01:59:57 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 94.7 KB, free 2004.1 MB)
19/04/24 01:59:57 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 35.5 KB, free 2004.1 MB)
19/04/24 01:59:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on varlet1.fyre.ibm.com:39491 (size: 35.5 KB, free: 2004.5 MB)
19/04/24 01:59:57 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1039
19/04/24 01:59:57 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at map at IOCommon.scala:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
19/04/24 01:59:57 INFO YarnScheduler: Adding task set 1.0 with 8 tasks
19/04/24 01:59:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, hurds2.fyre.ibm.com, executor 2, partition 0, NODE_LOCAL, 7660 bytes)
19/04/24 01:59:57 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 9, hurds3.fyre.ibm.com, executor 1, partition 1, NODE_LOCAL, 7660 bytes)
19/04/24 01:59:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hurds3.fyre.ibm.com:36857 (size: 35.5 KB, free: 2004.5 MB)
19/04/24 01:59:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hurds2.fyre.ibm.com:44282 (size: 35.5 KB, free: 2004.5 MB)
19/04/24 01:59:57 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.16.191.74:59668
19/04/24 01:59:57 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.16.191.17:39118
19/04/24 01:59:57 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 10, hurds3.fyre.ibm.com, executor 1, partition 2, NODE_LOCAL, 7660 bytes)
19/04/24 01:59:57 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 9) in 466 ms on hurds3.fyre.ibm.com (executor 1) (1/8)
19/04/24 01:59:57 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 11, hurds3.fyre.ibm.com, executor 1, partition 3, NODE_LOCAL, 7660 bytes)
19/04/24 01:59:57 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 10) in 133 ms on hurds3.fyre.ibm.com (executor 1) (2/8)
19/04/24 01:59:57 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 12, hurds2.fyre.ibm.com, executor 2, partition 4, NODE_LOCAL, 7660 bytes)
19/04/24 01:59:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 8) in 605 ms on hurds2.fyre.ibm.com (executor 2) (3/8)
19/04/24 01:59:57 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 13, hurds3.fyre.ibm.com, executor 1, partition 5, NODE_LOCAL, 7660 bytes)
19/04/24 01:59:57 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 11) in 106 ms on hurds3.fyre.ibm.com (executor 1) (4/8)
19/04/24 01:59:57 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 14, hurds2.fyre.ibm.com, executor 2, partition 6, NODE_LOCAL, 7660 bytes)
19/04/24 01:59:57 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 12) in 144 ms on hurds2.fyre.ibm.com (executor 2) (5/8)
19/04/24 01:59:58 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 15, hurds3.fyre.ibm.com, executor 1, partition 7, NODE_LOCAL, 7660 bytes)
19/04/24 01:59:58 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 13) in 84 ms on hurds3.fyre.ibm.com (executor 1) (6/8)
19/04/24 01:59:58 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 14) in 133 ms on hurds2.fyre.ibm.com (executor 2) (7/8)
19/04/24 01:59:58 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 15) in 95 ms on hurds3.fyre.ibm.com (executor 1) (8/8)
19/04/24 01:59:58 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
19/04/24 01:59:58 INFO DAGScheduler: ResultStage 1 (runJob at SparkHadoopWriter.scala:78) finished in 0,915 s
19/04/24 01:59:58 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 4,344425 s
19/04/24 01:59:58 INFO SparkHadoopWriter: Job job_20190424015953_0006 committed.
19/04/24 01:59:58 INFO AbstractConnector: Stopped Spark@cc6460c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19/04/24 01:59:58 INFO SparkUI: Stopped Spark web UI at http://varlet1.fyre.ibm.com:4040
19/04/24 01:59:58 INFO YarnClientSchedulerBackend: Interrupting monitor thread
19/04/24 01:59:58 INFO YarnClientSchedulerBackend: Shutting down all executors
19/04/24 01:59:58 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
19/04/24 01:59:58 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
19/04/24 01:59:58 INFO YarnClientSchedulerBackend: Stopped
19/04/24 01:59:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/04/24 01:59:58 INFO MemoryStore: MemoryStore cleared
19/04/24 01:59:58 INFO BlockManager: BlockManager stopped
19/04/24 01:59:58 INFO BlockManagerMaster: BlockManagerMaster stopped
19/04/24 01:59:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/04/24 01:59:58 INFO SparkContext: Successfully stopped SparkContext
19/04/24 01:59:58 INFO ShutdownHookManager: Shutdown hook called
19/04/24 01:59:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-aee05420-0e7d-4ea9-b738-c2c509945417
19/04/24 01:59:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-25141d1c-54e1-4434-8ea8-583875a25a94
finish ScalaSparkWordcount bench
Prepare micro.dfsioe ...
Exec script: /home/bench/HiBench/bin/workloads/micro/dfsioe/prepare/prepare.sh
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/micro/dfsioe.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start HadoopPrepareDFSIOE bench
hdfs rm -r: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -rm -r -skipTrash hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input
rm: `hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input': No such file or directory
Submit MapReduce Job: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop jar /home/bench/HiBench/autogen/target/autogen-7.1-SNAPSHOT.jar org.apache.hadoop.fs.dfsioe.TestDFSIOEnh -Dmapreduce.map.java.opts="-Dtest.build.data=hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input -Xmx3276m" -Dmapreduce.reduce.java.opts="-Dtest.build.data=hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input -Xmx6553m" -Dtest.build.data=hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input -write -skipAnalyze -nrFiles 16 -fileSize 1 -bufferSize 4096
TestFDSIO.0.0.4 Enhanced Version
19/04/24 02:00:02 INFO dfsioe.TestDFSIOEnh: nrFiles = 16
19/04/24 02:00:02 INFO dfsioe.TestDFSIOEnh: fileSize (MB) = 1
19/04/24 02:00:02 INFO dfsioe.TestDFSIOEnh: bufferSize = 4096
19/04/24 02:00:04 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 02:00:04 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 02:00:04 INFO dfsioe.TestDFSIOEnh: maximum concurrent maps = 10
19/04/24 02:00:04 INFO dfsioe.TestDFSIOEnh: creating control file: 1 mega bytes, 16 files
19/04/24 02:00:04 INFO dfsioe.DfsioeConfig: Found 'test.build.data' in Hadoop configuration, value is 'hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input'
19/04/24 02:00:04 INFO dfsioe.DfsioeConfig: Setting testRootDir to 'hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input'
19/04/24 02:00:05 INFO dfsioe.TestDFSIOEnh: created control files for: 16 files
19/04/24 02:00:05 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 02:00:05 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 02:00:05 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 02:00:05 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 02:00:06 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/bench/.staging/job_1556063405565_0017
19/04/24 02:00:06 INFO mapred.FileInputFormat: Total input files to process : 16
19/04/24 02:00:06 INFO mapreduce.JobSubmitter: number of splits:16
19/04/24 02:00:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1556063405565_0017
19/04/24 02:00:07 INFO mapreduce.JobSubmitter: Executing with tokens: []
19/04/24 02:00:07 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/04/24 02:00:07 INFO impl.YarnClientImpl: Submitted application application_1556063405565_0017
19/04/24 02:00:07 INFO mapreduce.Job: The url to track the job: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0017/
19/04/24 02:00:07 INFO mapreduce.Job: Running job: job_1556063405565_0017
19/04/24 02:00:12 INFO mapreduce.Job: Job job_1556063405565_0017 running in uber mode : false
19/04/24 02:00:12 INFO mapreduce.Job:  map 0% reduce 0%
19/04/24 02:00:18 INFO mapreduce.Job:  map 6% reduce 0%
19/04/24 02:00:20 INFO mapreduce.Job:  map 25% reduce 0%
19/04/24 02:00:21 INFO mapreduce.Job:  map 56% reduce 0%
19/04/24 02:00:22 INFO mapreduce.Job:  map 63% reduce 0%
19/04/24 02:00:27 INFO mapreduce.Job:  map 81% reduce 0%
19/04/24 02:00:28 INFO mapreduce.Job:  map 94% reduce 0%
19/04/24 02:00:29 INFO mapreduce.Job:  map 100% reduce 0%
19/04/24 02:00:30 INFO mapreduce.Job:  map 100% reduce 100%
19/04/24 02:00:30 INFO mapreduce.Job: Job job_1556063405565_0017 completed successfully
19/04/24 02:00:30 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=92887
		FILE: Number of bytes written=4167198
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=4060
		HDFS: Number of bytes written=16868108
		HDFS: Number of read operations=69
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=18
	Job Counters 
		Launched map tasks=16
		Launched reduce tasks=1
		Data-local map tasks=16
		Total time spent by all maps in occupied slots (ms)=345604
		Total time spent by all reduces in occupied slots (ms)=46808
		Total time spent by all map tasks (ms)=86401
		Total time spent by all reduce tasks (ms)=5851
		Total vcore-milliseconds taken by all map tasks=86401
		Total vcore-milliseconds taken by all reduce tasks=5851
		Total megabyte-milliseconds taken by all map tasks=353898496
		Total megabyte-milliseconds taken by all reduce tasks=47931392
	Map-Reduce Framework
		Map input records=16
		Map output records=224
		Map output bytes=92257
		Map output materialized bytes=92977
		Input split bytes=2262
		Combine input records=0
		Combine output records=0
		Reduce input groups=134
		Reduce shuffle bytes=92977
		Reduce input records=224
		Reduce output records=134
		Spilled Records=448
		Shuffled Maps =16
		Failed Shuffles=0
		Merged Map outputs=16
		GC time elapsed (ms)=2228
		CPU time spent (ms)=43840
		Physical memory (bytes) snapshot=38252941312
		Virtual memory (bytes) snapshot=97352060928
		Total committed heap usage (bytes)=37555798016
		Peak Map Physical memory (bytes)=2383745024
		Peak Map Virtual memory (bytes)=5517672448
		Peak Reduce Physical memory (bytes)=215392256
		Peak Reduce Virtual memory (bytes)=9111818240
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1798
	File Output Format Counters 
		Bytes Written=90892
/home/bench/HiBench/bin/functions/workload_functions.sh: line 74: kill: (14846) - Nie ma takiego procesu
finish HadoopPrepareDFSIOE bench
Run micro/dfsioe/hadoop
Exec script: /home/bench/HiBench/bin/workloads/micro/dfsioe/hadoop/run.sh
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/micro/dfsioe.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start HadoopDfsioe-read bench
hdfs rm -r: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -rm -r -skipTrash hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input/io_read
rm: `hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input/io_read': No such file or directory
hdfs rm -r: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -rm -r -skipTrash hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input/_*
rm: `hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input/_*': No such file or directory
hdfs du -s: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -du -s hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input
Submit MapReduce Job: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop jar /home/bench/HiBench/autogen/target/autogen-7.1-SNAPSHOT.jar org.apache.hadoop.fs.dfsioe.TestDFSIOEnh -Dmapreduce.map.java.opts="-Dtest.build.data=hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input -Xmx3276m" -Dmapreduce.reduce.java.opts="-Dtest.build.data=hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input -Xmx6553m" -read -nrFiles 16 -fileSize 1 -bufferSize 131072 -plotInteval 1000 -sampleUnit m -sampleInteval 200 -sumThreshold 0.5 -tputReportTotal -Dtest.build.data=hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input -resFile /home/bench/HiBench/report/dfsioe/hadoop/conf/../result_read.txt -tputFile /home/bench/HiBench/report/dfsioe/hadoop/conf/../throughput_read.csv
TestFDSIO.0.0.4 Enhanced Version
19/04/24 02:00:40 INFO dfsioe.TestDFSIOEnh: nrFiles = 16
19/04/24 02:00:40 INFO dfsioe.TestDFSIOEnh: fileSize (MB) = 1
19/04/24 02:00:40 INFO dfsioe.TestDFSIOEnh: bufferSize = 131072
19/04/24 02:00:41 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 02:00:41 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 02:00:41 INFO dfsioe.TestDFSIOEnh: maximum concurrent maps = 10
19/04/24 02:00:41 INFO dfsioe.TestDFSIOEnh: creating control file: 1 mega bytes, 16 files
19/04/24 02:00:41 INFO dfsioe.DfsioeConfig: Found 'test.build.data' in System properties, value is 'hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input'
19/04/24 02:00:41 INFO dfsioe.DfsioeConfig: Setting testRootDir to 'hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input'
19/04/24 02:00:42 INFO dfsioe.TestDFSIOEnh: created control files for: 16 files
19/04/24 02:00:42 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 02:00:42 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 02:00:42 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 02:00:42 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 02:00:43 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/bench/.staging/job_1556063405565_0018
19/04/24 02:00:43 INFO mapred.FileInputFormat: Total input files to process : 16
19/04/24 02:00:43 INFO mapreduce.JobSubmitter: number of splits:16
19/04/24 02:00:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1556063405565_0018
19/04/24 02:00:43 INFO mapreduce.JobSubmitter: Executing with tokens: []
19/04/24 02:00:43 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/04/24 02:00:44 INFO impl.YarnClientImpl: Submitted application application_1556063405565_0018
19/04/24 02:00:44 INFO mapreduce.Job: The url to track the job: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0018/
19/04/24 02:00:44 INFO mapreduce.Job: Running job: job_1556063405565_0018
19/04/24 02:00:51 INFO mapreduce.Job: Job job_1556063405565_0018 running in uber mode : false
19/04/24 02:00:51 INFO mapreduce.Job:  map 0% reduce 0%
19/04/24 02:00:57 INFO mapreduce.Job:  map 13% reduce 0%
19/04/24 02:00:58 INFO mapreduce.Job:  map 50% reduce 0%
19/04/24 02:00:59 INFO mapreduce.Job:  map 56% reduce 0%
19/04/24 02:01:01 INFO mapreduce.Job:  map 63% reduce 0%
19/04/24 02:01:02 INFO mapreduce.Job:  map 69% reduce 0%
19/04/24 02:01:03 INFO mapreduce.Job:  map 75% reduce 0%
19/04/24 02:01:04 INFO mapreduce.Job:  map 100% reduce 0%
19/04/24 02:01:08 INFO mapreduce.Job:  map 100% reduce 100%
19/04/24 02:01:08 INFO mapreduce.Job: Job job_1556063405565_0018 completed successfully
19/04/24 02:01:08 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=92972
		FILE: Number of bytes written=4163900
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=16781276
		HDFS: Number of bytes written=90991
		HDFS: Number of read operations=85
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=16
		Launched reduce tasks=1
		Data-local map tasks=16
		Total time spent by all maps in occupied slots (ms)=290412
		Total time spent by all reduces in occupied slots (ms)=25536
		Total time spent by all map tasks (ms)=72603
		Total time spent by all reduce tasks (ms)=3192
		Total vcore-milliseconds taken by all map tasks=72603
		Total vcore-milliseconds taken by all reduce tasks=3192
		Total megabyte-milliseconds taken by all map tasks=297381888
		Total megabyte-milliseconds taken by all reduce tasks=26148864
	Map-Reduce Framework
		Map input records=16
		Map output records=224
		Map output bytes=92342
		Map output materialized bytes=93062
		Input split bytes=2262
		Combine input records=0
		Combine output records=0
		Reduce input groups=134
		Reduce shuffle bytes=93062
		Reduce input records=224
		Reduce output records=134
		Spilled Records=448
		Shuffled Maps =16
		Failed Shuffles=0
		Merged Map outputs=16
		GC time elapsed (ms)=1857
		CPU time spent (ms)=34960
		Physical memory (bytes) snapshot=38271918080
		Virtual memory (bytes) snapshot=97307725824
		Total committed heap usage (bytes)=37673762816
		Peak Map Physical memory (bytes)=2385334272
		Peak Map Virtual memory (bytes)=5516152832
		Peak Reduce Physical memory (bytes)=231829504
		Peak Reduce Virtual memory (bytes)=9106812928
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1798
	File Output Format Counters 
		Bytes Written=90991
19/04/24 02:01:09 INFO dfsioe.TestDFSIOEnh: ----- TestDFSIO ----- : read
19/04/24 02:01:09 INFO dfsioe.TestDFSIOEnh:            Date & time: Wed Apr 24 02:01:08 CEST 2019
19/04/24 02:01:09 INFO dfsioe.TestDFSIOEnh:        Number of files: 16
19/04/24 02:01:09 INFO dfsioe.TestDFSIOEnh: Total MBytes processed: 16
19/04/24 02:01:09 INFO dfsioe.TestDFSIOEnh:      Throughput mb/sec: 36.7816091954023
19/04/24 02:01:09 INFO dfsioe.TestDFSIOEnh: Average IO rate mb/sec: 52.7861213684082
19/04/24 02:01:09 INFO dfsioe.TestDFSIOEnh:  IO rate std deviation: 26.083257625911664
19/04/24 02:01:09 INFO dfsioe.TestDFSIOEnh:     Test exec time sec: 26.086
19/04/24 02:01:09 INFO dfsioe.TestDFSIOEnh: 
19/04/24 02:01:09 INFO dfsioe.TestDFSIOEnh: -- Extended Metrics --   : read
19/04/24 02:01:09 INFO dfsioe.TestDFSIOEnh: Result file name         : /home/bench/HiBench/report/dfsioe/hadoop/conf/../throughput_read.csv
19/04/24 02:01:09 INFO dfsioe.TestDFSIOEnh: Sampling overhead        : 0.6896552%
19/04/24 02:01:09 INFO dfsioe.TestDFSIOEnh: Reference Start Time     : 1556064042864
19/04/24 02:01:09 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 02:01:09 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 02:01:09 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/bench/.staging/job_1556063405565_0019
19/04/24 02:01:09 INFO input.FileInputFormat: Total input files to process : 1
19/04/24 02:01:09 INFO mapreduce.JobSubmitter: number of splits:1
19/04/24 02:01:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1556063405565_0019
19/04/24 02:01:09 INFO mapreduce.JobSubmitter: Executing with tokens: []
19/04/24 02:01:10 INFO impl.YarnClientImpl: Submitted application application_1556063405565_0019
19/04/24 02:01:10 INFO mapreduce.Job: The url to track the job: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0019/
19/04/24 02:01:10 INFO mapreduce.Job: Running job: job_1556063405565_0019
19/04/24 02:01:21 INFO mapreduce.Job: Job job_1556063405565_0019 running in uber mode : false
19/04/24 02:01:21 INFO mapreduce.Job:  map 0% reduce 0%
19/04/24 02:01:28 INFO mapreduce.Job:  map 100% reduce 0%
19/04/24 02:01:34 INFO mapreduce.Job:  map 100% reduce 100%
19/04/24 02:01:34 INFO mapreduce.Job: Job job_1556063405565_0019 completed successfully
19/04/24 02:01:34 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=137490
		FILE: Number of bytes written=743983
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=91135
		HDFS: Number of bytes written=12353
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=18880
		Total time spent by all reduces in occupied slots (ms)=23632
		Total time spent by all map tasks (ms)=4720
		Total time spent by all reduce tasks (ms)=2954
		Total vcore-milliseconds taken by all map tasks=4720
		Total vcore-milliseconds taken by all reduce tasks=2954
		Total megabyte-milliseconds taken by all map tasks=19333120
		Total megabyte-milliseconds taken by all reduce tasks=24199168
	Map-Reduce Framework
		Map input records=134
		Map output records=4128
		Map output bytes=129228
		Map output materialized bytes=137490
		Input split bytes=144
		Combine input records=0
		Combine output records=0
		Reduce input groups=16
		Reduce shuffle bytes=137490
		Reduce input records=4128
		Reduce output records=448
		Spilled Records=8256
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=160
		CPU time spent (ms)=3310
		Physical memory (bytes) snapshot=2612944896
		Virtual memory (bytes) snapshot=14620028928
		Total committed heap usage (bytes)=2534408192
		Peak Map Physical memory (bytes)=2377031680
		Peak Map Virtual memory (bytes)=5510017024
		Peak Reduce Physical memory (bytes)=235913216
		Peak Reduce Virtual memory (bytes)=9110011904
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=90991
	File Output Format Counters 
		Bytes Written=12353
19/04/24 02:01:34 INFO dfsioe.TestDFSIOEnh: remote report file hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input/_merged_reports.txt merged.
19/04/24 02:01:34 INFO dfsioe.TestDFSIOEnh: Aggregated throughput results are unavailable, because the concurrency of Mappers is too low and consequently the aggregated throughput measurement is not very meaningful
19/04/24 02:01:34 INFO dfsioe.TestDFSIOEnh: Please adjust your test workload and try again.
/home/bench/HiBench/bin/functions/workload_functions.sh: line 74: kill: (15452) - Nie ma takiego procesu
finish HadoopDfsioe-read bench
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/micro/dfsioe.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start HadoopDfsioe-write bench
hdfs rm -r: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -rm -r -skipTrash hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Output
rm: `hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Output': No such file or directory
hdfs du -s: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -du -s hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input
Submit MapReduce Job: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop jar /home/bench/HiBench/autogen/target/autogen-7.1-SNAPSHOT.jar org.apache.hadoop.fs.dfsioe.TestDFSIOEnh -Dmapreduce.map.java.opts="-Dtest.build.data=hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input -Xmx3276m" -Dmapreduce.reduce.java.opts="-Dtest.build.data=hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input -Xmx6553m" -write -nrFiles 16 -fileSize 1 -bufferSize 4096 -plotInteval 1000 -sampleUnit m -sampleInteval 200 -sumThreshold 0.5 -tputReportTotal -Dtest.build.data=hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input -resFile /home/bench/HiBench/report/dfsioe/hadoop/conf/../result_write.txt -tputFile /home/bench/HiBench/report/dfsioe/hadoop/conf/../throughput_write.csv
TestFDSIO.0.0.4 Enhanced Version
19/04/24 02:01:42 INFO dfsioe.TestDFSIOEnh: nrFiles = 16
19/04/24 02:01:42 INFO dfsioe.TestDFSIOEnh: fileSize (MB) = 1
19/04/24 02:01:42 INFO dfsioe.TestDFSIOEnh: bufferSize = 4096
19/04/24 02:01:44 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 02:01:44 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 02:01:44 INFO dfsioe.TestDFSIOEnh: maximum concurrent maps = 10
19/04/24 02:01:44 INFO dfsioe.TestDFSIOEnh: creating control file: 1 mega bytes, 16 files
19/04/24 02:01:44 INFO dfsioe.DfsioeConfig: Found 'test.build.data' in System properties, value is 'hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input'
19/04/24 02:01:44 INFO dfsioe.DfsioeConfig: Setting testRootDir to 'hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input'
19/04/24 02:01:46 INFO dfsioe.TestDFSIOEnh: created control files for: 16 files
19/04/24 02:01:46 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 02:01:46 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 02:01:46 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 02:01:46 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 02:01:46 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/bench/.staging/job_1556063405565_0020
19/04/24 02:01:46 INFO mapred.FileInputFormat: Total input files to process : 16
19/04/24 02:01:46 INFO mapreduce.JobSubmitter: number of splits:16
19/04/24 02:01:47 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1556063405565_0020
19/04/24 02:01:47 INFO mapreduce.JobSubmitter: Executing with tokens: []
19/04/24 02:01:47 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/04/24 02:01:47 INFO impl.YarnClientImpl: Submitted application application_1556063405565_0020
19/04/24 02:01:47 INFO mapreduce.Job: The url to track the job: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0020/
19/04/24 02:01:47 INFO mapreduce.Job: Running job: job_1556063405565_0020
19/04/24 02:01:53 INFO mapreduce.Job: Job job_1556063405565_0020 running in uber mode : false
19/04/24 02:01:53 INFO mapreduce.Job:  map 0% reduce 0%
19/04/24 02:01:59 INFO mapreduce.Job:  map 6% reduce 0%
19/04/24 02:02:01 INFO mapreduce.Job:  map 50% reduce 0%
19/04/24 02:02:02 INFO mapreduce.Job:  map 63% reduce 0%
19/04/24 02:02:04 INFO mapreduce.Job:  map 69% reduce 0%
19/04/24 02:02:06 INFO mapreduce.Job:  map 75% reduce 0%
19/04/24 02:02:07 INFO mapreduce.Job:  map 81% reduce 0%
19/04/24 02:02:08 INFO mapreduce.Job:  map 94% reduce 0%
19/04/24 02:02:09 INFO mapreduce.Job:  map 100% reduce 0%
19/04/24 02:02:10 INFO mapreduce.Job:  map 100% reduce 100%
19/04/24 02:02:10 INFO mapreduce.Job: Job job_1556063405565_0020 completed successfully
19/04/24 02:02:11 INFO mapreduce.Job: Counters: 54
	File System Counters
		FILE: Number of bytes read=92889
		FILE: Number of bytes written=4163751
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=4060
		HDFS: Number of bytes written=16868109
		HDFS: Number of read operations=69
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=18
	Job Counters 
		Launched map tasks=16
		Launched reduce tasks=1
		Data-local map tasks=14
		Rack-local map tasks=2
		Total time spent by all maps in occupied slots (ms)=365656
		Total time spent by all reduces in occupied slots (ms)=47792
		Total time spent by all map tasks (ms)=91414
		Total time spent by all reduce tasks (ms)=5974
		Total vcore-milliseconds taken by all map tasks=91414
		Total vcore-milliseconds taken by all reduce tasks=5974
		Total megabyte-milliseconds taken by all map tasks=374431744
		Total megabyte-milliseconds taken by all reduce tasks=48939008
	Map-Reduce Framework
		Map input records=16
		Map output records=224
		Map output bytes=92259
		Map output materialized bytes=92979
		Input split bytes=2262
		Combine input records=0
		Combine output records=0
		Reduce input groups=134
		Reduce shuffle bytes=92979
		Reduce input records=224
		Reduce output records=134
		Spilled Records=448
		Shuffled Maps =16
		Failed Shuffles=0
		Merged Map outputs=16
		GC time elapsed (ms)=2610
		CPU time spent (ms)=46120
		Physical memory (bytes) snapshot=38245613568
		Virtual memory (bytes) snapshot=97336803328
		Total committed heap usage (bytes)=37760794624
		Peak Map Physical memory (bytes)=2381283328
		Peak Map Virtual memory (bytes)=5516881920
		Peak Reduce Physical memory (bytes)=214315008
		Peak Reduce Virtual memory (bytes)=9107353600
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1798
	File Output Format Counters 
		Bytes Written=90893
19/04/24 02:02:11 INFO dfsioe.TestDFSIOEnh: ----- TestDFSIO ----- : write
19/04/24 02:02:11 INFO dfsioe.TestDFSIOEnh:            Date & time: Wed Apr 24 02:02:11 CEST 2019
19/04/24 02:02:11 INFO dfsioe.TestDFSIOEnh:        Number of files: 16
19/04/24 02:02:11 INFO dfsioe.TestDFSIOEnh: Total MBytes processed: 16
19/04/24 02:02:11 INFO dfsioe.TestDFSIOEnh:      Throughput mb/sec: 4.545454545454546
19/04/24 02:02:11 INFO dfsioe.TestDFSIOEnh: Average IO rate mb/sec: 5.385452747344971
19/04/24 02:02:11 INFO dfsioe.TestDFSIOEnh:  IO rate std deviation: 1.8722789239855382
19/04/24 02:02:11 INFO dfsioe.TestDFSIOEnh:     Test exec time sec: 24.893
19/04/24 02:02:11 INFO dfsioe.TestDFSIOEnh: 
19/04/24 02:02:11 INFO dfsioe.TestDFSIOEnh: -- Extended Metrics --   : write
19/04/24 02:02:11 INFO dfsioe.TestDFSIOEnh: Result file name         : /home/bench/HiBench/report/dfsioe/hadoop/conf/../throughput_write.csv
19/04/24 02:02:11 INFO dfsioe.TestDFSIOEnh: Sampling overhead        : 0.8238636%
19/04/24 02:02:11 INFO dfsioe.TestDFSIOEnh: Reference Start Time     : 1556064106191
19/04/24 02:02:11 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 02:02:11 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 02:02:11 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/bench/.staging/job_1556063405565_0021
19/04/24 02:02:11 INFO input.FileInputFormat: Total input files to process : 1
19/04/24 02:02:11 INFO mapreduce.JobSubmitter: number of splits:1
19/04/24 02:02:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1556063405565_0021
19/04/24 02:02:11 INFO mapreduce.JobSubmitter: Executing with tokens: []
19/04/24 02:02:11 INFO impl.YarnClientImpl: Submitted application application_1556063405565_0021
19/04/24 02:02:11 INFO mapreduce.Job: The url to track the job: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0021/
19/04/24 02:02:11 INFO mapreduce.Job: Running job: job_1556063405565_0021
19/04/24 02:02:16 INFO mapreduce.Job: Job job_1556063405565_0021 running in uber mode : false
19/04/24 02:02:16 INFO mapreduce.Job:  map 0% reduce 0%
19/04/24 02:02:22 INFO mapreduce.Job:  map 100% reduce 0%
19/04/24 02:02:29 INFO mapreduce.Job:  map 100% reduce 100%
19/04/24 02:02:30 INFO mapreduce.Job: Job job_1556063405565_0021 completed successfully
19/04/24 02:02:30 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=137394
		FILE: Number of bytes written=743789
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=91038
		HDFS: Number of bytes written=8902
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=15324
		Total time spent by all reduces in occupied slots (ms)=24392
		Total time spent by all map tasks (ms)=3831
		Total time spent by all reduce tasks (ms)=3049
		Total vcore-milliseconds taken by all map tasks=3831
		Total vcore-milliseconds taken by all reduce tasks=3049
		Total megabyte-milliseconds taken by all map tasks=15691776
		Total megabyte-milliseconds taken by all reduce tasks=24977408
	Map-Reduce Framework
		Map input records=134
		Map output records=4128
		Map output bytes=129132
		Map output materialized bytes=137394
		Input split bytes=145
		Combine input records=0
		Combine output records=0
		Reduce input groups=16
		Reduce shuffle bytes=137394
		Reduce input records=4128
		Reduce output records=416
		Spilled Records=8256
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=175
		CPU time spent (ms)=3410
		Physical memory (bytes) snapshot=2606817280
		Virtual memory (bytes) snapshot=14617919488
		Total committed heap usage (bytes)=2543321088
		Peak Map Physical memory (bytes)=2375856128
		Peak Map Virtual memory (bytes)=5510483968
		Peak Reduce Physical memory (bytes)=230961152
		Peak Reduce Virtual memory (bytes)=9107435520
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=90893
	File Output Format Counters 
		Bytes Written=8902
19/04/24 02:02:30 INFO dfsioe.TestDFSIOEnh: remote report file hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Dfsioe/Input/_merged_reports.txt merged.
19/04/24 02:02:30 INFO dfsioe.TestDFSIOEnh: Aggregated throughput results are unavailable, because the concurrency of Mappers is too low and consequently the aggregated throughput measurement is not very meaningful
19/04/24 02:02:30 INFO dfsioe.TestDFSIOEnh: Please adjust your test workload and try again.
/home/bench/HiBench/bin/functions/workload_functions.sh: line 74: kill: (16146) - Nie ma takiego procesu
finish HadoopDfsioe-write bench
Prepare sql.aggregation ...
Exec script: /home/bench/HiBench/bin/workloads/sql/aggregation/prepare/prepare.sh
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/sql/aggregation.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start HadoopPrepareAggregation bench
hdfs rm -r: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -rm -r -skipTrash hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Aggregation/Input
rm: `hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Aggregation/Input': No such file or directory
Pages:120, USERVISITS:1000
Submit MapReduce Job: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop jar /home/bench/HiBench/autogen/target/autogen-7.1-SNAPSHOT.jar HiBench.DataGen -t hive -b hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Aggregation -n Input -m 8 -r 8 -p 120 -v 1000 -o sequence
19/04/24 02:02:34 INFO HiBench.HiveData: Generating hive data files...
19/04/24 02:02:34 INFO HiBench.HiveData: Initializing hive date generator...
curIndex: 11, total: 12
19/04/24 02:02:37 INFO HiBench.Dummy: Creating dummy file hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Aggregation/temp/dummy with 8 slots...
19/04/24 02:02:37 INFO HiBench.HiveData: Creating table rankings...
19/04/24 02:02:37 INFO HiBench.HiveData: Running Job: Create rankings
19/04/24 02:02:37 INFO HiBench.HiveData: Pages file hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Aggregation/temp/dummy as input
19/04/24 02:02:37 INFO HiBench.HiveData: Rankings file hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Aggregation/Input/rankings as output
19/04/24 02:02:37 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 02:02:38 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 02:02:38 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 02:02:38 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 02:02:38 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
19/04/24 02:02:38 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/bench/.staging/job_1556063405565_0022
19/04/24 02:02:38 INFO mapred.FileInputFormat: Total input files to process : 1
19/04/24 02:02:38 INFO mapreduce.JobSubmitter: number of splits:8
19/04/24 02:02:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1556063405565_0022
19/04/24 02:02:38 INFO mapreduce.JobSubmitter: Executing with tokens: []
19/04/24 02:02:39 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/04/24 02:02:39 INFO impl.YarnClientImpl: Submitted application application_1556063405565_0022
19/04/24 02:02:39 INFO mapreduce.Job: The url to track the job: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0022/
19/04/24 02:02:39 INFO mapreduce.Job: Running job: job_1556063405565_0022
19/04/24 02:02:44 INFO mapreduce.Job: Job job_1556063405565_0022 running in uber mode : false
19/04/24 02:02:44 INFO mapreduce.Job:  map 0% reduce 0%
19/04/24 02:02:50 INFO mapreduce.Job:  map 13% reduce 0%
19/04/24 02:02:51 INFO mapreduce.Job:  map 38% reduce 0%
19/04/24 02:02:52 INFO mapreduce.Job:  map 100% reduce 0%
19/04/24 02:02:57 INFO mapreduce.Job:  map 100% reduce 38%
19/04/24 02:03:01 INFO mapreduce.Job:  map 100% reduce 75%
19/04/24 02:03:05 INFO mapreduce.Job:  map 100% reduce 100%
19/04/24 02:03:05 INFO mapreduce.Job: Job job_1556063405565_0022 completed successfully
19/04/24 02:03:06 INFO mapreduce.Job: Counters: 54
	File System Counters
		FILE: Number of bytes read=18516
		FILE: Number of bytes written=3808448
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1055
		HDFS: Number of bytes written=7360
		HDFS: Number of read operations=64
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=16
	Job Counters 
		Launched map tasks=8
		Launched reduce tasks=8
		Other local map tasks=8
		Total time spent by all maps in occupied slots (ms)=158828
		Total time spent by all reduces in occupied slots (ms)=177280
		Total time spent by all map tasks (ms)=39707
		Total time spent by all reduce tasks (ms)=22160
		Total vcore-milliseconds taken by all map tasks=39707
		Total vcore-milliseconds taken by all reduce tasks=22160
		Total megabyte-milliseconds taken by all map tasks=162639872
		Total megabyte-milliseconds taken by all reduce tasks=181534720
	Map-Reduce Framework
		Map input records=8
		Map output records=1063
		Map output bytes=17651
		Map output materialized bytes=18852
		Input split bytes=976
		Combine input records=1063
		Combine output records=944
		Reduce input groups=120
		Reduce shuffle bytes=18852
		Reduce input records=944
		Reduce output records=120
		Spilled Records=1888
		Shuffled Maps =64
		Failed Shuffles=0
		Merged Map outputs=64
		GC time elapsed (ms)=1534
		CPU time spent (ms)=26610
		Physical memory (bytes) snapshot=20882460672
		Virtual memory (bytes) snapshot=116947603456
		Total committed heap usage (bytes)=20329791488
		Peak Map Physical memory (bytes)=2384097280
		Peak Map Virtual memory (bytes)=5517152256
		Peak Reduce Physical memory (bytes)=236400640
		Peak Reduce Virtual memory (bytes)=9107918848
	HiBench.Counters
		BYTES_DATA_GENERATED=8814
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=79
	File Output Format Counters 
		Bytes Written=7360
19/04/24 02:03:06 INFO HiBench.HiveData: Finished Running Job: Create rankings
19/04/24 02:03:06 INFO HiBench.HiveData: Creating user visits...
19/04/24 02:03:06 INFO HiBench.HiveData: Running Job: Create uservisits
19/04/24 02:03:06 INFO HiBench.HiveData: Dummy file hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Aggregation/temp/dummy as input
19/04/24 02:03:06 INFO HiBench.HiveData: Rankings file hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Aggregation/Input/rankings as input
19/04/24 02:03:06 INFO HiBench.HiveData: Ouput file hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Aggregation/Input/uservisits
19/04/24 02:03:06 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 02:03:06 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 02:03:06 INFO client.RMProxy: Connecting to ResourceManager at a1.fyre.ibm.com/172.16.151.219:8050
19/04/24 02:03:06 INFO client.AHSProxy: Connecting to Application History server at aa1.fyre.ibm.com/172.16.151.62:10200
19/04/24 02:03:06 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
19/04/24 02:03:06 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/bench/.staging/job_1556063405565_0023
19/04/24 02:03:06 INFO mapred.FileInputFormat: Total input files to process : 8
19/04/24 02:03:06 INFO mapred.FileInputFormat: Total input files to process : 1
19/04/24 02:03:06 INFO mapreduce.JobSubmitter: number of splits:16
19/04/24 02:03:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1556063405565_0023
19/04/24 02:03:06 INFO mapreduce.JobSubmitter: Executing with tokens: []
19/04/24 02:03:06 INFO impl.YarnClientImpl: Submitted application application_1556063405565_0023
19/04/24 02:03:06 INFO mapreduce.Job: The url to track the job: http://a1.fyre.ibm.com:8088/proxy/application_1556063405565_0023/
19/04/24 02:03:06 INFO mapreduce.Job: Running job: job_1556063405565_0023
19/04/24 02:03:13 INFO mapreduce.Job: Job job_1556063405565_0023 running in uber mode : false
19/04/24 02:03:13 INFO mapreduce.Job:  map 0% reduce 0%
19/04/24 02:03:19 INFO mapreduce.Job:  map 13% reduce 0%
19/04/24 02:03:21 INFO mapreduce.Job:  map 19% reduce 0%
19/04/24 02:03:22 INFO mapreduce.Job:  map 50% reduce 0%
19/04/24 02:03:23 INFO mapreduce.Job:  map 63% reduce 0%
19/04/24 02:03:25 INFO mapreduce.Job:  map 75% reduce 0%
19/04/24 02:03:27 INFO mapreduce.Job:  map 88% reduce 0%
19/04/24 02:03:28 INFO mapreduce.Job:  map 100% reduce 0%
19/04/24 02:03:29 INFO mapreduce.Job:  map 100% reduce 25%
19/04/24 02:03:32 INFO mapreduce.Job:  map 100% reduce 38%
19/04/24 02:03:33 INFO mapreduce.Job:  map 100% reduce 63%
19/04/24 02:03:35 INFO mapreduce.Job:  map 100% reduce 75%
19/04/24 02:03:37 INFO mapreduce.Job:  map 100% reduce 100%
19/04/24 02:03:38 INFO mapreduce.Job: Job job_1556063405565_0023 completed successfully
19/04/24 02:03:38 INFO mapreduce.Job: Counters: 55
	File System Counters
		FILE: Number of bytes read=16081
		FILE: Number of bytes written=5690280
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=11687
		HDFS: Number of bytes written=190183
		HDFS: Number of read operations=96
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=16
	Job Counters 
		Launched map tasks=16
		Launched reduce tasks=8
		Other local map tasks=8
		Data-local map tasks=8
		Total time spent by all maps in occupied slots (ms)=310936
		Total time spent by all reduces in occupied slots (ms)=204976
		Total time spent by all map tasks (ms)=77734
		Total time spent by all reduce tasks (ms)=25622
		Total vcore-milliseconds taken by all map tasks=77734
		Total vcore-milliseconds taken by all reduce tasks=25622
		Total megabyte-milliseconds taken by all map tasks=318398464
		Total megabyte-milliseconds taken by all reduce tasks=209895424
	Map-Reduce Framework
		Map input records=128
		Map output records=1120
		Map output bytes=18221
		Map output materialized bytes=16801
		Input split bytes=4248
		Combine input records=1120
		Combine output records=751
		Reduce input groups=120
		Reduce shuffle bytes=16801
		Reduce input records=751
		Reduce output records=1000
		Spilled Records=1502
		Shuffled Maps =128
		Failed Shuffles=0
		Merged Map outputs=128
		GC time elapsed (ms)=2328
		CPU time spent (ms)=46720
		Physical memory (bytes) snapshot=39887011840
		Virtual memory (bytes) snapshot=161044373504
		Total committed heap usage (bytes)=39058931712
		Peak Map Physical memory (bytes)=2384650240
		Peak Map Virtual memory (bytes)=5514641408
		Peak Reduce Physical memory (bytes)=237998080
		Peak Reduce Virtual memory (bytes)=9111531520
	HiBench.Counters
		BYTES_DATA_GENERATED=179620
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=190183
19/04/24 02:03:38 INFO HiBench.HiveData: Finished Running Job: Create uservisits
19/04/24 02:03:38 INFO HiBench.HiveData: Closing hive data generator...
finish HadoopPrepareAggregation bench
Run sql/aggregation/hadoop
Exec script: /home/bench/HiBench/bin/workloads/sql/aggregation/hadoop/run.sh
patching args=
Parsing conf: /home/bench/HiBench/conf/hadoop.conf
Parsing conf: /home/bench/HiBench/conf/hibench.conf
Parsing conf: /home/bench/HiBench/conf/spark.conf
Parsing conf: /home/bench/HiBench/conf/workloads/sql/aggregation.conf
probe sleep jar: /usr/hdp/current/hadoop-client/../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start HadoopAggregation bench
Export env: HADOOP_EXECUTABLE=/usr/hdp/current/hadoop-client/bin/hadoop
hdfs rm -r: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -rm -r -skipTrash hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Aggregation/Output
rm: `hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Aggregation/Output': No such file or directory
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.0.0-78/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.0.0-78/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Connecting to jdbc:hive2://a1.fyre.ibm.com:2181,aa1.fyre.ibm.com:2181,hurds1.fyre.ibm.com:2181/default;password=bench;serviceDiscoveryMode=zooKeeper;user=bench;zooKeeperNamespace=hiveserver2
19/04/24 02:03:48 [main]: INFO jdbc.HiveConnection: Connected to hurds1.fyre.ibm.com:10000
Connected to: Apache Hive (version 3.1.0.3.1.0.0-78)
Driver: Hive JDBC (version 3.1.0.3.1.0.0-78)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://a1.fyre.ibm.com:2181,aa1.fyre> USE DEFAULT;
Error: Error while compiling statement: FAILED: HiveAccessControlException Permission denied: user [bench] does not have [USE] privilege on [default] (state=42000,code=40000)
Closing: 0: jdbc:hive2://a1.fyre.ibm.com:2181,aa1.fyre.ibm.com:2181,hurds1.fyre.ibm.com:2181/default;password=bench;serviceDiscoveryMode=zooKeeper;user=bench;zooKeeperNamespace=hiveserver2
hdfs du -s: /usr/hdp/current/hadoop-client/bin/hadoop --config /usr/hdp/current/hadoop-client/etc/hadoop fs -du -s hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Aggregation/Output
du: `hdfs://a1.fyre.ibm.com:8020/tmp/hibench/HiBench/Aggregation/Output': No such file or directory
/home/bench/HiBench/bin/functions/workload_functions.sh: line 85: $3: nieustawiona zmienna
ERROR: sql/aggregation/hadoop failed to run successfully.
